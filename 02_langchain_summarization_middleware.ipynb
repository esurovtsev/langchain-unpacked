{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain 1.0 Middleware – Summarization for Long Doc Q&A Chats\n",
    "\n",
    "In this lesson you’ll watch a normal doc Q&A agent slowly dig its own grave with a huge conversation history, and then see how the **built-in Summarization middleware** bails it out.\n",
    "\n",
    "The focus here isn’t *\"what is summarization?\"* — you already know that. The focus is: **what you get for free when you plug in the prebuilt middleware** instead of hand-rolling your own trimming logic.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "We’ll use the same LangChain 1.0-style `create_agent` API from the first middleware lesson, but now point it at a tiny slice of *LangChain docs* and let it answer questions about them.\n",
    "\n",
    "First, imports and environment:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # Load OPENAI_API_KEY and LangSmith settings if present\n",
    "\n",
    "import uuid\n",
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "from langchain.agents import create_agent\n",
    "from langchain.agents.middleware import SummarizationMiddleware\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langchain.messages import HumanMessage\n",
    "from helpers import draw_mermaid_png\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. LangChain docs via MCP\n",
    "\n",
    "Instead of hard-coding a docs string, we'll talk to the real LangChain docs via an MCP server.\n",
    "\n",
    "That server exposes tools the agent can call to look things up in the docs, and those tool calls will show up in the message history that our summarization middleware compresses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MCP_SERVERS = {\n",
    "    \"langchain-docs\": {\n",
    "        \"transport\": \"streamable_http\",\n",
    "        \"url\": \"https://docs.langchain.com/mcp/\",\n",
    "    }\n",
    "}\n",
    "\n",
    "mcp_client = MultiServerMCPClient(MCP_SERVERS)\n",
    "LANGCHAIN_DOCS_TOOLS = await mcp_client.get_tools()\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are a helpful assistant answering questions about the LangChain and LangGraph ecosystem.\n",
    "You have access to tools that query the official LangChain docs via MCP.\n",
    "Always call the docs tools when you need information, and base your answers only on what they return.\n",
    "If something is not covered, say you don't know.\n",
    "\n",
    "Keep answers short, concrete, and oriented toward developers.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97564505",
   "metadata": {},
   "outputs": [],
   "source": [
    "LANGCHAIN_DOCS_TOOLS[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Baseline doc Q&A agent (no summarization)\n",
    "\n",
    "First we build a normal agent with **no middleware**.\n",
    "\n",
    "We’ll feed it a series of questions about these docs and watch the conversation history grow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_checkpointer = InMemorySaver()\n",
    "\n",
    "baseline_agent = create_agent(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    tools=LANGCHAIN_DOCS_TOOLS,  # MCP tools that query the live LangChain docs\n",
    "    system_prompt=SYSTEM_PROMPT,\n",
    "    checkpointer=baseline_checkpointer,\n",
    ")\n",
    "\n",
    "draw_mermaid_png(baseline_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448a6bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_questions = [\n",
    "    \"What is LangChain 1.0 in one sentence?\",\n",
    "    \"How does LangChain 1.0 relate to LangGraph?\",\n",
    "    \"What role do tools play in an agentic application?\",\n",
    "    \"What is middleware used for in LangChain 1.0?\",\n",
    "    \"Where in the flow does summarization middleware act?\",\n",
    "    \"Why might I need summarization for long-running chats?\",\n",
    "    \"How is summarization middleware different from just trimming messages?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Generate a long-ish conversation without summarization\n",
    "\n",
    "Here we pretend to be a curious user asking several follow-up questions.\n",
    "\n",
    "The key part: we keep passing the **full message history** back into the agent. No trimming, no summarization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b0c218",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_thread_id = str(uuid.uuid4())\n",
    "baseline_config = {\"configurable\": {\"thread_id\": baseline_thread_id}}\n",
    "\n",
    "baseline_messages = []\n",
    "\n",
    "for question in user_questions:\n",
    "    baseline_messages.append(HumanMessage(question))\n",
    "    response = await baseline_agent.ainvoke({\"messages\": baseline_messages}, config=baseline_config)\n",
    "    baseline_messages = response[\"messages\"]\n",
    "\n",
    "len(baseline_messages)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Inspect the raw history\n",
    "\n",
    "Let’s peek at the full, uncompressed history so you can see exactly what the agent is carrying around without any summarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2dde40",
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in baseline_messages:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Plug in the prebuilt Summarization middleware\n",
    "\n",
    "Now we let the **middleware** do the heavy lifting.\n",
    "\n",
    "Two key ideas in this implementation:\n",
    "\n",
    "- It **triggers** when the total token count or message count crosses the `trigger` threshold (here: `(\"tokens\", 2000)`).\n",
    "- It **compresses** by keeping a small tail of recent messages verbatim (`keep=(\"messages\", 4)`) and replacing everything before that with a single summary message that lives in the history.\n",
    "\n",
    "In other setups you can configure `trigger` and `keep` with `(\"tokens\", N)`, `(\"messages\", N)`, or `(\"fraction\", f)`, and for `trigger` even pass a list like `[ (\"fraction\", 0.8), (\"messages\", 100) ]`.\n",
    "\n",
    "We keep the agent code exactly the same and bolt this behavior on via the `middleware` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_checkpointer = InMemorySaver()\n",
    "\n",
    "summary_agent = create_agent(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    tools=LANGCHAIN_DOCS_TOOLS,\n",
    "    system_prompt=SYSTEM_PROMPT,\n",
    "    checkpointer=summary_checkpointer,\n",
    "    middleware=[\n",
    "        SummarizationMiddleware(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            trigger=(\"tokens\", 2000),   # set small for demo so it triggers quickly\n",
    "            keep=(\"messages\", 4),      # always keep the most recent 4 messages verbatim\n",
    "            trim_tokens_to_summarize=None,\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "\n",
    "draw_mermaid_png(summary_agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Re-run the same conversation with summarization enabled\n",
    "\n",
    "We’ll ask the **same questions** in the same order, but now the middleware is watching the message history and is allowed to rewrite it when the token threshold is crossed.\n",
    "\n",
    "The important part to notice: the **summary does not show up as the user-visible answer**. It shows up as a new message inside `state[\"messages\"]` *before* the next model call.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_thread_id = str(uuid.uuid4())\n",
    "summary_config = {\"configurable\": {\"thread_id\": summary_thread_id}}\n",
    "\n",
    "summary_messages = []\n",
    "\n",
    "for question in user_questions:\n",
    "    summary_messages.append(HumanMessage(question))\n",
    "    response = await summary_agent.ainvoke({\"messages\": summary_messages}, config=summary_config)\n",
    "    summary_messages = response[\"messages\"]\n",
    "\n",
    "len(summary_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bf29ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in summary_messages:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The takeaway: by adding a single middleware entry, we turned an ever-growing transcript into a **summary + small tail**, without touching the agent logic itself.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Customizing what the summary focuses on\n",
    "\n",
    "The prebuilt middleware also lets you steer **how** it summarizes, via the `summary_prompt` parameter.\n",
    "\n",
    "In this section we keep the same trigger/keep configuration, but change the prompt to do something more opinionated:\n",
    "\n",
    "- Focus only on what the **human** has been saying.\n",
    "- Infer their underlying intent and goals.\n",
    "- Suggest where the conversation is likely heading next.\n",
    "\n",
    "This turns the summary from a neutral recap into a kind of \"intent + guidance\" overlay for the chat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUSTOM_SUMMARY_PROMPT = \"\"\"\n",
    "You are summarizing a conversation between a developer and an assistant about LangChain 1.0.\n",
    "\n",
    "Your job is not to recap every fact. Instead, you are trying to read the developer's mind a bit.\n",
    "\n",
    "Rules:\n",
    "- Focus only on the Human messages (what the user typed). Treat assistant and tool messages only as weak clues about the user's intent.\n",
    "- Infer what the human is really trying to do, ask, or fix over the course of the chat.\n",
    "- Capture any frustration, constraints, or strong preferences the human expressed.\n",
    "- Keep a light, slightly funny tone, but stay respectful and concise.\n",
    "- Never say the conversation is too long to summarize or that you cannot summarize it.\n",
    "- Even if the messages look long, repetitive, or truncated, still summarize what you can see.\n",
    "\n",
    "Produce three short sections:\n",
    "1. User storyline  – a narrative summary of what the human has been doing/asking so far.\n",
    "2. Intent & goals  – what the human seems to ultimately want to achieve in this session.\n",
    "3. Likely next steps  – 2–4 concrete suggestions for where this conversation is heading or what the assistant should help with next.\n",
    "\n",
    "Conversation messages to summarize:\n",
    "{messages}\n",
    "\n",
    "Return only the summary text in those three sections, using short bullet points where helpful.\n",
    "\"\"\"\n",
    "\n",
    "custom_summary_checkpointer = InMemorySaver()\n",
    "\n",
    "custom_summary_agent = create_agent(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    tools=LANGCHAIN_DOCS_TOOLS,\n",
    "    system_prompt=SYSTEM_PROMPT,\n",
    "    checkpointer=custom_summary_checkpointer,\n",
    "    middleware=[\n",
    "        SummarizationMiddleware(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            trigger=(\"tokens\", 2000),\n",
    "            keep=(\"messages\", 4),\n",
    "            summary_prompt=CUSTOM_SUMMARY_PROMPT,\n",
    "            trim_tokens_to_summarize=None,\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "\n",
    "draw_mermaid_png(custom_summary_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f481996",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_thread_id = str(uuid.uuid4())\n",
    "summary_config = {\"configurable\": {\"thread_id\": summary_thread_id}}\n",
    "\n",
    "summary_messages = []\n",
    "\n",
    "for question in user_questions:\n",
    "    summary_messages.append(HumanMessage(question))\n",
    "    response = await custom_summary_agent.ainvoke({\"messages\": summary_messages}, config=summary_config)\n",
    "    summary_messages = response[\"messages\"]\n",
    "\n",
    "len(summary_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7021c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in summary_messages:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can keep iterating on `summary_prompt` until the summary matches the level of detail your application needs. The important point: **all of this customization lives in middleware**, not in the agent loop itself.\n",
    "\n",
    "That’s the power of the new LangChain 1.0 middleware system: one agent, many cross-cutting behaviors you can plug in or swap out without rewriting your core logic.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db98bf63",
   "metadata": {},
   "source": [
    "## Reference Links\n",
    "\n",
    "**1. SummarizationMiddleware – Python docs**\n",
    "\n",
    "https://docs.langchain.com/oss/python/langchain/middleware/built-in\n",
    "\n",
    "→ Overview of the built-in Summarization middleware, its configuration options (`model`, `trigger`, `keep`, `summary_prompt`, `trim_tokens_to_summarize`), and example usage.\n",
    "\n",
    "**2. Middleware in LangChain 1.0**\n",
    "\n",
    "https://docs.langchain.com/oss/python/langchain/middleware\n",
    "\n",
    "→ High-level guide to the middleware system, how hooks work, and how to combine multiple middlewares in a single agent.\n",
    "\n",
    "**3. Context engineering & summarization example**\n",
    "\n",
    "https://docs.langchain.com/oss/javascript/langchain/context-engineering\n",
    "\n",
    "→ Conceptual walk-through of summarization as a context-engineering pattern, including how summaries replace old messages and keep a short tail of recent turns.\n",
    "\n",
    "**4. Model Context Protocol (MCP) in LangChain**\n",
    "\n",
    "https://docs.langchain.com/oss/python/langchain/mcp\n",
    "\n",
    "→ How to connect to MCP servers with `langchain-mcp-adapters`, load tools with `MultiServerMCPClient`, and plug MCP-backed tools into LangChain/LangGraph agents."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
