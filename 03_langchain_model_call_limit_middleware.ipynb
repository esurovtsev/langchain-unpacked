{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain 1.0 Middleware – Taming the Overenthusiastic Agent\n",
    "\n",
    "You know that feeling when you ask someone a simple question and they go on a 20-minute tangent? Agents can do the same thing. They'll happily loop forever, calling the model again and again, burning through your API budget like it's free pizza at a hackathon.\n",
    "\n",
    "In this lesson, we'll watch an agent go a little overboard, then see how **ModelCallLimitMiddleware** puts a leash on it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "Same setup as the previous lesson. We'll use the MCP-powered LangChain docs tools so the agent has something meaty to research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "import uuid\n",
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "from langchain.agents import create_agent\n",
    "from langchain.agents.middleware import ModelCallLimitMiddleware\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langchain.messages import HumanMessage\n",
    "from helpers import draw_mermaid_png\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MCP_SERVERS = {\n",
    "    \"langchain-docs\": {\n",
    "        \"transport\": \"streamable_http\",\n",
    "        \"url\": \"https://docs.langchain.com/mcp/\",\n",
    "    }\n",
    "}\n",
    "\n",
    "mcp_client = MultiServerMCPClient(MCP_SERVERS)\n",
    "LANGCHAIN_DOCS_TOOLS = await mcp_client.get_tools()\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are a thorough research assistant answering questions about LangChain and LangGraph.\n",
    "You have access to tools that query the official LangChain docs.\n",
    "\n",
    "When asked to research a topic, be comprehensive. Search the docs multiple times if needed\n",
    "to gather all relevant information. Don't stop after just one search.\n",
    "\"\"\"\n",
    "\n",
    "MODEL = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0,\n",
    "    model_kwargs={\"parallel_tool_calls\": False},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Overenthusiastic Agent (No Limits)\n",
    "\n",
    "First, let's build an agent with no guardrails. We'll give it a broad research task and count how many model calls it makes.\n",
    "\n",
    "Here's the part that matters: the system prompt encourages thoroughness. The agent will keep searching and synthesizing until it feels done. Which might take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/png;base64,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\" style=\"display:block;margin:0 auto;width:100%;max-width:min(90vw, 300px);max-height:70vh;height:auto\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "baseline_agent = create_agent(\n",
    "    model=MODEL,\n",
    "    tools=LANGCHAIN_DOCS_TOOLS,\n",
    "    system_prompt=SYSTEM_PROMPT,\n",
    "    checkpointer=InMemorySaver(),\n",
    ")\n",
    "\n",
    "draw_mermaid_png(baseline_agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's give it something to chew on. A nice, broad research question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total messages in conversation: 12\n"
     ]
    }
   ],
   "source": [
    "RESEARCH_TASK = \"\"\"\n",
    "You are researching LangChain's built-in middleware.\n",
    "Work in at least 5 clearly separated AI steps. After each step, send an answer\n",
    "before continuing.\n",
    "\n",
    "1. Step 1: Plan your research on built-in middleware (no tool calls yet). Output only the plan.\n",
    "2. Step 2: Use the docs search tool to find a complete list of built-in middlewares. Output what you found.\n",
    "3. Step 3: For each middleware in that list, run a separate docs search focused on that middleware and update your notes.\n",
    "4. Step 4: Run another docs search focused only on limitations / failure modes / “gotchas” for these middlewares. Update your notes.\n",
    "5. Step 5: Only after all previous steps, write the final consolidated answer for me.\n",
    "\n",
    "At the end of each step, explicitly write: `STEP X DONE`, then continue with the next step.\n",
    "\"\"\"\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": str(uuid.uuid4())}}\n",
    "\n",
    "response = await baseline_agent.ainvoke(\n",
    "    {\"messages\": [HumanMessage(RESEARCH_TASK)]},\n",
    "    config=config\n",
    ")\n",
    "\n",
    "# Count how many messages we ended up with\n",
    "print(f\"Total messages in conversation: {len(response['messages'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "\n",
      "You are researching LangChain's built-in middleware.\n",
      "Work in at least 5 clearly separated AI steps. After each step, send an answer\n",
      "before continuing.\n",
      "\n",
      "1. Step 1: Plan your research on built-in middleware (no tool calls yet). Output only the plan.\n",
      "2. Step 2: Use the docs search tool to find a complete list of built-in middlewares. Output what you found.\n",
      "3. Step 3: For each middleware in that list, run a separate docs search focused on that middleware and update your notes.\n",
      "4. Step 4: Run another docs search focused only on limitations / failure modes / “gotchas” for these middlewares. Update your notes.\n",
      "5. Step 5: Only after all previous steps, write the final consolidated answer for me.\n",
      "\n",
      "At the end of each step, explicitly write: `STEP X DONE`, then continue with the next step.\n",
      "\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "### Step 1: Plan your research on built-in middleware\n",
      "\n",
      "1. **Define Middleware**: Understand what middleware is in the context of LangChain and its purpose.\n",
      "2. **Identify Built-in Middleware**: Search for a complete list of built-in middleware provided by LangChain.\n",
      "3. **Examine Each Middleware**: For each middleware identified, gather detailed information about its functionality, use cases, and examples.\n",
      "4. **Investigate Limitations**: Look for any limitations, failure modes, or common issues associated with the built-in middleware.\n",
      "5. **Consolidate Findings**: Compile all the gathered information into a comprehensive overview of LangChain's built-in middleware.\n",
      "\n",
      "STEP 1 DONE. \n",
      "\n",
      "### Step 2: Use the docs search tool to find a complete list of built-in middlewares\n",
      "\n",
      "Now, I will search the LangChain documentation for a complete list of built-in middleware. \n",
      "\n",
      "Executing search...\n",
      "Tool Calls:\n",
      "  SearchDocsByLangChain (call_gIxxTcITMKHtZZmBes7s3nCM)\n",
      " Call ID: call_gIxxTcITMKHtZZmBes7s3nCM\n",
      "  Args:\n",
      "    query: built-in middleware\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: SearchDocsByLangChain\n",
      "\n",
      "[\"Title: Built-in middleware\\nLink: https://docs.langchain.com/oss/javascript/langchain/middleware/built-in\\nContent: LangChain provides prebuilt middleware for common use cases. Each middleware is production-ready and configurable for your specific needs.\\n\\nPrebuilt middleware for common agent use cases\\n\\nProvider-specific middleware\\nThese middleware are optimized for specific LLM providers. See each provider's documentation for full details and examples. Anthropic Prompt caching, bash tool, text editor, memory, and file search middleware for Claude models. Edit the source of this page on GitHub. Connect these docs programmatically to Claude, VSCode, and more via MCP for real-time answers.\\n\\n\", \"Title: Built-in middleware\\nLink: https://docs.langchain.com/oss/python/langchain/middleware/built-in\\nContent: LangChain provides prebuilt middleware for common use cases. Each middleware is production-ready and configurable for your specific needs.\\n\\nPrebuilt middleware for common agent use cases\\n\\nTo-do list\\nEquip agents with task planning and tracking capabilities for complex multi-step tasks. To-do lists are useful for the following: Complex multi-step tasks requiring coordination across multiple tools. Long-running operations where progress visibility is important. This middleware automatically provides agents with a write_todos tool and system prompts to guide effective task planning. API reference: TodoListMiddleware Watch this video guide demonstrating To-do List middleware behavior. system_prompt Custom system prompt for guiding todo usage. Uses built-in prompt if not specified. tool_description Custom description for the write_todos tool. Uses built-in description if not specified.\\n\\n\", \"Title: How to add custom middleware\\nLink: https://docs.langchain.com/langsmith/custom-middleware\\nContent: When deploying agents to LangSmith, you can add custom middleware to your server to handle concerns like logging request metrics, injecting or checking headers, and enforcing security policies without modifying core server logic. This works the same way as adding custom routes . You just need to provide your own Starlette app (including FastAPI , FastHTML and other compatible apps). Adding middleware lets you intercept and modify requests and responses globally across your deployment, whether they're hitting your custom endpoints or the built-in LangSmith APIs. Below is an example using FastAPI. \\\"Python only\\\"\\nWe currently only support custom middleware in Python deployments with langgraph-api>=0.0.26.\\n\\nHow to add custom middleware\\nWhen deploying agents to LangSmith, you can add custom middleware to your server to handle concerns like logging request metrics, injecting or checking headers, and enforcing security policies without modifying core server logic. This works the same way as adding custom routes . You just need to provide your own Starlette app (including FastAPI , FastHTML and other compatible apps). Adding middleware lets you intercept and modify requests and responses globally across your deployment, whether they're hitting your custom endpoints or the built-in LangSmith APIs. Below is an example using FastAPI. \\\"Python only\\\"\\nWe currently only support custom middleware in Python deployments with langgraph-api>=0.0.26.\\n\\nCustomize middleware ordering\\nBy default, custom middleware runs before authentication logic. To run custom middleware after authentication, set middleware_order to auth_first in your http configuration. (This customization is supported starting with API server v0.4.35 and later.)\\n\\n\", \"Title: Custom middleware\\nLink: https://docs.langchain.com/oss/python/langchain/middleware/custom\\nContent: Build custom middleware by implementing hooks that run at specific points in the agent execution flow.\\n\\nCustom middleware\\nBuild custom middleware by implementing hooks that run at specific points in the agent execution flow.\\n\\nCreate middleware\\nYou can create middleware in two ways: Decorator-based middleware Quick and simple for single-hook middleware. Use decorators to wrap individual functions. Class-based middleware More powerful for complex middleware with multiple hooks or configuration.\\n\\n\", \"Title: Custom middleware\\nLink: https://docs.langchain.com/oss/javascript/langchain/middleware/custom\\nContent: Build custom middleware by implementing hooks that run at specific points in the agent execution flow.\\n\\nCustom middleware\\nBuild custom middleware by implementing hooks that run at specific points in the agent execution flow.\\n\\nAdditional resources\\nMiddleware API reference Built-in middleware Testing agents Edit the source of this page on GitHub. Connect these docs programmatically to Claude, VSCode, and more via MCP for real-time answers.\\n\\n\", \"Title: Deep Agents Middleware\\nLink: https://docs.langchain.com/oss/javascript/deepagents/middleware\\nContent: Deep agents are built with a modular middleware architecture. Deep agents have access to: A planning tool A filesystem for storing context and long-term memories The ability to spawn subagents Each feature is implemented as separate middleware. When you create a deep agent with create_deep_agent , we automatically attach TodoListMiddleware , FilesystemMiddleware , and SubAgentMiddleware to your agent. Middleware is composable—you can add as many or as few middleware to an agent as needed. You can use any middleware independently. The following sections explain what each middleware provides.\\n\\nDeep agents are built with a modular middleware architecture. Deep agents have access to: A planning tool A filesystem for storing context and long-term memories The ability to spawn subagents Each feature is implemented as separate middleware. When you create a deep agent with create_deep_agent , we automatically attach TodoListMiddleware , FilesystemMiddleware , and SubAgentMiddleware to your agent. graph LR\\n    Agent[create_deep_agent] --> Todo[TodoList]\\n    Agent --> FS[Filesystem]\\n    Agent --> Sub[SubAgent]\\n\\n    Todo --> Tools[Agent Tools]\\n    FS --> Tools\\n    Sub --> Tools\\n\\nUnderstand the middleware that powers deep agents\\n\\n\", \"Title: Deep Agents Middleware\\nLink: https://docs.langchain.com/oss/python/deepagents/middleware\\nContent: Deep agents are built with a modular middleware architecture. Deep agents have access to: A planning tool A filesystem for storing context and long-term memories The ability to spawn subagents Each feature is implemented as separate middleware. When you create a deep agent with create_deep_agent , we automatically attach TodoListMiddleware , FilesystemMiddleware , and SubAgentMiddleware to your agent. Middleware is composable—you can add as many or as few middleware to an agent as needed. You can use any middleware independently. The following sections explain what each middleware provides.\\n\\nDeep agents are built with a modular middleware architecture. Deep agents have access to: A planning tool A filesystem for storing context and long-term memories The ability to spawn subagents Each feature is implemented as separate middleware. When you create a deep agent with create_deep_agent , we automatically attach TodoListMiddleware , FilesystemMiddleware , and SubAgentMiddleware to your agent. graph LR\\n    Agent[create_deep_agent] --> Todo[TodoList]\\n    Agent --> FS[Filesystem]\\n    Agent --> Sub[SubAgent]\\n\\n    Todo --> Tools[Agent Tools]\\n    FS --> Tools\\n    Sub --> Tools\\n\\nUnderstand the middleware that powers deep agents\\n\\n\", \"Title: Provider-specific middleware\\nLink: https://docs.langchain.com/oss/python/integrations/middleware/index\\nContent: Provider-specific middleware\\nMiddleware designed for specific providers. Learn more about middleware. Provider Middleware available Anthropic Prompt caching, bash tool, text editor, memory, and file search OpenAI Content moderation Edit the source of this page on GitHub. Connect these docs programmatically to Claude, VSCode, and more via MCP for real-time answers.\\n\\nMiddleware designed for specific providers. Learn more about middleware. Provider Middleware available Anthropic Prompt caching, bash tool, text editor, memory, and file search OpenAI Content moderation Edit the source of this page on GitHub. Connect these docs programmatically to Claude, VSCode, and more via MCP for real-time answers.\\n\\n\", \"Title: Provider-specific middleware\\nLink: https://docs.langchain.com/oss/javascript/integrations/middleware/index\\nContent: Provider-specific middleware\\nMiddleware designed for specific providers. Learn more about middleware. Provider Middleware available Anthropic Prompt caching Edit the source of this page on GitHub. Connect these docs programmatically to Claude, VSCode, and more via MCP for real-time answers.\\n\\nMiddleware designed for specific providers. Learn more about middleware. Provider Middleware available Anthropic Prompt caching Edit the source of this page on GitHub. Connect these docs programmatically to Claude, VSCode, and more via MCP for real-time answers.\\n\\n\", \"Title: Middleware\\nLink: https://docs.langchain.com/oss/javascript/releases/langchain-v1\\nContent: Middleware is the defining feature of createAgent . It makes createAgent highly customizable, raising the ceiling for what you can build. Great agents require context engineering : getting the right information to the model at the right time. Middleware helps you control dynamic prompts, conversation summarization, selective tool access, state management, and guardrails through a composable abstraction.\\n\\nCustom middleware\\nYou can also build custom middleware to fit your specific needs. Build custom middleware by implementing any of these hooks using the create function: Hook When it runs Use cases beforeAgent Before calling the agent Load memory, validate input beforeModel Before each LLM call Update prompts, trim messages wrapModelCall Around each LLM call Intercept and modify requests/responses wrapToolCall Around each tool call Intercept and modify tool execution afterModel After each LLM response Validate output, apply guardrails afterAgent After agent completes Save results, cleanup Example custom middleware: For more information, see the complete middleware guide.\\n\\nCustom middleware\\nYou can also build custom middleware to fit your specific needs. Build custom middleware by implementing any of these hooks using the create function: Hook When it runs Use cases beforeAgent Before calling the agent Load memory, validate input beforeModel Before each LLM call Update prompts, trim messages wrapModelCall Around each LLM call Intercept and modify requests/responses wrapToolCall Around each tool call Intercept and modify tool execution afterModel After each LLM response Validate output, apply guardrails afterAgent After agent completes Save results, cleanup Example custom middleware: import { createMiddleware } from \\\"langchain\\\";\\n\\nconst contextSchema = z.object({\\n  userExpertise: z.enum([\\\"beginner\\\", \\\"expert\\\"]).default(\\\"beginner\\\"),\\n})\\n\\nconst expertiseBasedToolMiddleware = createMiddleware({\\n  wrapModelCall: async (request, handler) => {\\n    const userLevel = request.runtime.context.userExpertise;\\n    if (userLevel === \\\"expert\\\") {\\n      const tools = [advancedSearch, dataAnalysis];\\n      return handler(\\n        request.replace(\\\"openai:gpt-5\\\", tools)\\n      );\\n    }\\n    const tools = [simpleSearch, basicCalculator];\\n    return handler(\\n      request.replace(\\\"openai:gpt-5-nano\\\", tools)\\n    );\\n  },\\n});\\n\\nconst agent = createAgent({\\n  model: \\\"claude-sonnet-4-5-20250929\\\",\\n  tools: [simpleSearch, advancedSearch, basicCalculator, dataAnalysis],\\n  middleware: [expertiseBasedToolMiddleware],\\n  contextSchema,\\n});\\n\\n\"]\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I found the following information regarding built-in middleware in LangChain:\n",
      "\n",
      "1. **Overview**: LangChain provides prebuilt middleware for common use cases, which are production-ready and configurable for specific needs.\n",
      "\n",
      "2. **Types of Middleware**:\n",
      "   - **Provider-specific Middleware**: Optimized for specific LLM providers. Examples include:\n",
      "     - **Anthropic**: Includes prompt caching, bash tool, text editor, memory, and file search middleware for Claude models.\n",
      "     - **OpenAI**: Includes content moderation middleware.\n",
      "\n",
      "3. **Example Middleware**:\n",
      "   - **To-do List Middleware**: This middleware equips agents with task planning and tracking capabilities for complex multi-step tasks. It automatically provides agents with a `write_todos` tool and system prompts to guide effective task planning.\n",
      "\n",
      "For more detailed information, you can refer to the following links:\n",
      "- [Built-in middleware (Python)](https://docs.langchain.com/oss/python/langchain/middleware/built-in)\n",
      "- [Built-in middleware (JavaScript)](https://docs.langchain.com/oss/javascript/langchain/middleware/built-in)\n",
      "- [Provider-specific middleware (Python)](https://docs.langchain.com/oss/python/integrations/middleware/index)\n",
      "- [Provider-specific middleware (JavaScript)](https://docs.langchain.com/oss/javascript/integrations/middleware/index)\n",
      "\n",
      "STEP 2 DONE.\n",
      "\n",
      "### Step 3: For each middleware in that list, run a separate docs search focused on that middleware and update your notes\n",
      "\n",
      "Now, I will search for detailed information on each middleware identified in the previous step. \n",
      "\n",
      "1. **To-do List Middleware**\n",
      "2. **Provider-specific Middleware (Anthropic)**\n",
      "3. **Provider-specific Middleware (OpenAI)**\n",
      "\n",
      "I will start with the To-do List Middleware. \n",
      "\n",
      "Executing search...\n",
      "Tool Calls:\n",
      "  SearchDocsByLangChain (call_8lO3qAEBKLMwaaGsEu9xX7Tn)\n",
      " Call ID: call_8lO3qAEBKLMwaaGsEu9xX7Tn\n",
      "  Args:\n",
      "    query: To-do List Middleware\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: SearchDocsByLangChain\n",
      "\n",
      "[\"Title: To-do list middleware\\nLink: https://docs.langchain.com/oss/python/deepagents/middleware\\nContent: Planning is integral to solving complex problems. If you've used Claude Code recently, you'll notice how it writes out a to-do list before tackling complex, multi-part tasks. You'll also notice how it can adapt and update this to-do list on the fly as more information comes in. TodoListMiddleware provides your agent with a tool specifically for updating this to-do list. Before and while it executes a multi-part task, the agent is prompted to use the write_todos tool to keep track of what it's doing and what still needs to be done.\\n\\nPlanning is integral to solving complex problems. If you've used Claude Code recently, you'll notice how it writes out a to-do list before tackling complex, multi-part tasks. You'll also notice how it can adapt and update this to-do list on the fly as more information comes in. TodoListMiddleware provides your agent with a tool specifically for updating this to-do list. Before and while it executes a multi-part task, the agent is prompted to use the write_todos tool to keep track of what it's doing and what still needs to be done. from langchain.agents import create_agent\\nfrom langchain.agents.middleware import TodoListMiddleware\\n\\n# TodoListMiddleware is included by default in create_deep_agent\\n# You can customize it if building a custom agent\\nagent = create_agent(\\n    model=\\\"claude-sonnet-4-5-20250929\\\",\\n    # Custom planning instructions can be added via middleware\\n    middleware=[\\n        TodoListMiddleware(\\n            system_prompt=\\\"Use the write_todos tool to...\\\"  # Optional: Custom addition to the system prompt\\n        ),\\n    ],\\n)\\n\\nDeep Agents Middleware\\nDeep agents are built with a modular middleware architecture. Deep agents have access to: A planning tool A filesystem for storing context and long-term memories The ability to spawn subagents Each feature is implemented as separate middleware. When you create a deep agent with create_deep_agent , we automatically attach TodoListMiddleware , FilesystemMiddleware , and SubAgentMiddleware to your agent. Middleware is composable—you can add as many or as few middleware to an agent as needed. You can use any middleware independently. The following sections explain what each middleware provides.\\n\\n\", \"Title: To-do list middleware\\nLink: https://docs.langchain.com/oss/javascript/deepagents/middleware\\nContent: Planning is integral to solving complex problems. If you've used Claude Code recently, you'll notice how it writes out a to-do list before tackling complex, multi-part tasks. You'll also notice how it can adapt and update this to-do list on the fly as more information comes in. TodoListMiddleware provides your agent with a tool specifically for updating this to-do list. Before and while it executes a multi-part task, the agent is prompted to use the write_todos tool to keep track of what it's doing and what still needs to be done.\\n\\nPlanning is integral to solving complex problems. If you've used Claude Code recently, you'll notice how it writes out a to-do list before tackling complex, multi-part tasks. You'll also notice how it can adapt and update this to-do list on the fly as more information comes in. TodoListMiddleware provides your agent with a tool specifically for updating this to-do list. Before and while it executes a multi-part task, the agent is prompted to use the write_todos tool to keep track of what it's doing and what still needs to be done. import { createAgent, todoListMiddleware } from \\\"langchain\\\";\\n\\n// todoListMiddleware is included by default in createDeepAgent\\n// You can customize it if building a custom agent\\nconst agent = createAgent({\\n  model: \\\"claude-sonnet-4-5-20250929\\\",\\n  middleware: [\\n    todoListMiddleware({\\n      // Optional: Custom addition to the system prompt\\n      systemPrompt: \\\"Use the write_todos tool to...\\\",\\n    }),\\n  ],\\n});\\n\\nDeep Agents Middleware\\nDeep agents are built with a modular middleware architecture. Deep agents have access to: A planning tool A filesystem for storing context and long-term memories The ability to spawn subagents Each feature is implemented as separate middleware. When you create a deep agent with create_deep_agent , we automatically attach TodoListMiddleware , FilesystemMiddleware , and SubAgentMiddleware to your agent. Middleware is composable—you can add as many or as few middleware to an agent as needed. You can use any middleware independently. The following sections explain what each middleware provides.\\n\\n\", \"Title: To-do list\\nLink: https://docs.langchain.com/oss/javascript/langchain/middleware/built-in\\nContent: Equip agents with task planning and tracking capabilities for complex multi-step tasks. To-do lists are useful for the following: Complex multi-step tasks requiring coordination across multiple tools. Long-running operations where progress visibility is important. This middleware automatically provides agents with a write_todos tool and system prompts to guide effective task planning. Watch this video guide demonstrating To-do List middleware behavior. No configuration options available (uses defaults).\\n\\nEquip agents with task planning and tracking capabilities for complex multi-step tasks. To-do lists are useful for the following: Complex multi-step tasks requiring coordination across multiple tools. Long-running operations where progress visibility is important. This middleware automatically provides agents with a write_todos tool and system prompts to guide effective task planning. import { createAgent, todoListMiddleware } from \\\"langchain\\\";\\n\\nconst agent = createAgent({\\n  model: \\\"gpt-4o\\\",\\n  tools: [readFile, writeFile, runTests],\\n  middleware: [todoListMiddleware()],\\n});\\n\\nBuilt-in middleware\\nLangChain provides prebuilt middleware for common use cases. Each middleware is production-ready and configurable for your specific needs.\\n\\n\", \"Title: To-do list\\nLink: https://docs.langchain.com/oss/python/langchain/middleware/built-in\\nContent: Equip agents with task planning and tracking capabilities for complex multi-step tasks. To-do lists are useful for the following: Complex multi-step tasks requiring coordination across multiple tools. Long-running operations where progress visibility is important. This middleware automatically provides agents with a write_todos tool and system prompts to guide effective task planning. API reference: TodoListMiddleware Watch this video guide demonstrating To-do List middleware behavior. system_prompt {\\\"body\\\":\\\"system_prompt\\\",\\\"type\\\":\\\"string\\\"}\\n\\nEquip agents with task planning and tracking capabilities for complex multi-step tasks. To-do lists are useful for the following: Complex multi-step tasks requiring coordination across multiple tools. Long-running operations where progress visibility is important. This middleware automatically provides agents with a write_todos tool and system prompts to guide effective task planning. API reference: TodoListMiddleware Watch this video guide demonstrating To-do List middleware behavior. system_prompt Custom system prompt for guiding todo usage. Uses built-in prompt if not specified. tool_description {\\\"body\\\":\\\"tool_description\\\",\\\"type\\\":\\\"string\\\"}\\n\\nEquip agents with task planning and tracking capabilities for complex multi-step tasks. To-do lists are useful for the following: Complex multi-step tasks requiring coordination across multiple tools. Long-running operations where progress visibility is important. This middleware automatically provides agents with a write_todos tool and system prompts to guide effective task planning. API reference: TodoListMiddleware Watch this video guide demonstrating To-do List middleware behavior. system_prompt Custom system prompt for guiding todo usage. Uses built-in prompt if not specified. tool_description Custom description for the write_todos tool. Uses built-in description if not specified.\\n\\n\", \"Title: How to add custom middleware\\nLink: https://docs.langchain.com/langsmith/custom-middleware\\nContent: When deploying agents to LangSmith, you can add custom middleware to your server to handle concerns like logging request metrics, injecting or checking headers, and enforcing security policies without modifying core server logic. This works the same way as adding custom routes . You just need to provide your own Starlette app (including FastAPI , FastHTML and other compatible apps). Adding middleware lets you intercept and modify requests and responses globally across your deployment, whether they're hitting your custom endpoints or the built-in LangSmith APIs. Below is an example using FastAPI. \\\"Python only\\\"\\nWe currently only support custom middleware in Python deployments with langgraph-api>=0.0.26.\\n\\nHow to add custom middleware\\nWhen deploying agents to LangSmith, you can add custom middleware to your server to handle concerns like logging request metrics, injecting or checking headers, and enforcing security policies without modifying core server logic. This works the same way as adding custom routes . You just need to provide your own Starlette app (including FastAPI , FastHTML and other compatible apps). Adding middleware lets you intercept and modify requests and responses globally across your deployment, whether they're hitting your custom endpoints or the built-in LangSmith APIs. Below is an example using FastAPI. \\\"Python only\\\"\\nWe currently only support custom middleware in Python deployments with langgraph-api>=0.0.26.\\n\\nCustomize middleware ordering\\nBy default, custom middleware runs before authentication logic. To run custom middleware after authentication, set middleware_order to auth_first in your http configuration. (This customization is supported starting with API server v0.4.35 and later.)\\n\\n\", \"Title: Provider-specific middleware\\nLink: https://docs.langchain.com/oss/python/integrations/middleware/index\\nContent: Provider-specific middleware\\nMiddleware designed for specific providers. Learn more about middleware. Provider Middleware available Anthropic Prompt caching, bash tool, text editor, memory, and file search OpenAI Content moderation Edit the source of this page on GitHub. Connect these docs programmatically to Claude, VSCode, and more via MCP for real-time answers.\\n\\nMiddleware designed for specific providers. Learn more about middleware. Provider Middleware available Anthropic Prompt caching, bash tool, text editor, memory, and file search OpenAI Content moderation Edit the source of this page on GitHub. Connect these docs programmatically to Claude, VSCode, and more via MCP for real-time answers.\\n\\n\", \"Title: Provider-specific middleware\\nLink: https://docs.langchain.com/oss/javascript/integrations/middleware/index\\nContent: Provider-specific middleware\\nMiddleware designed for specific providers. Learn more about middleware. Provider Middleware available Anthropic Prompt caching Edit the source of this page on GitHub. Connect these docs programmatically to Claude, VSCode, and more via MCP for real-time answers.\\n\\nMiddleware designed for specific providers. Learn more about middleware. Provider Middleware available Anthropic Prompt caching Edit the source of this page on GitHub. Connect these docs programmatically to Claude, VSCode, and more via MCP for real-time answers.\\n\\n\", \"Title: Custom middleware\\nLink: https://docs.langchain.com/oss/python/langchain/middleware/custom\\nContent: Build custom middleware by implementing hooks that run at specific points in the agent execution flow.\\n\\nCustom middleware\\nBuild custom middleware by implementing hooks that run at specific points in the agent execution flow.\\n\\nCreate middleware\\nYou can create middleware in two ways: Decorator-based middleware Quick and simple for single-hook middleware. Use decorators to wrap individual functions. Class-based middleware More powerful for complex middleware with multiple hooks or configuration.\\n\\n\", \"Title: Custom middleware\\nLink: https://docs.langchain.com/oss/javascript/langchain/middleware/custom\\nContent: Build custom middleware by implementing hooks that run at specific points in the agent execution flow.\\n\\nCustom middleware\\nBuild custom middleware by implementing hooks that run at specific points in the agent execution flow.\\n\\nCreate middleware\\nUse the createMiddleware function to define custom middleware: import { createMiddleware } from \\\"langchain\\\";\\n\\nconst loggingMiddleware = createMiddleware({\\n  name: \\\"LoggingMiddleware\\\",\\n  beforeModel: (state) => {\\n    console.log(`About to call model with ${state.messages.length} messages`);\\n    return;\\n  },\\n  afterModel: (state) => {\\n    const lastMessage = state.messages[state.messages.length - 1];\\n    console.log(`Model returned: ${lastMessage.content}`);\\n    return;\\n  },\\n});\\n\\n\", \"Title: Middleware\\nLink: https://docs.langchain.com/oss/javascript/langchain/agents\\nContent: Middleware provides powerful extensibility for customizing agent behavior at different stages of execution. You can use middleware to: Process state before the model is called (e.g., message trimming, context injection) Modify or validate the model's response (e.g., guardrails, content filtering) Handle tool execution errors with custom logic Implement dynamic model selection based on state or context Add custom logging, monitoring, or analytics Middleware integrates seamlessly into the agent's execution, allowing you to intercept and modify data flow at key points without changing the core agent logic. For comprehensive middleware documentation including hooks like beforeModel , afterModel , and wrapToolCall , see Middleware. Edit the source of this page on GitHub. Connect these docs programmatically to Claude, VSCode, and more via MCP for real-time answers.\\n\\nDynamic system prompt\\nFor more advanced use cases where you need to modify the system prompt based on runtime context or agent state, you can use middleware. For more details on message types and formatting, see Messages . For comprehensive middleware documentation, see .\\n\\nDynamic model\\nDynamic models are selected at runtime based on the current state and context. This enables sophisticated routing logic and cost optimization. To use a dynamic model, create middleware with wrapModelCall that modifies the model in the request: import { ChatOpenAI } from \\\"@langchain/openai\\\";\\nimport { createAgent, create } from \\\"langchain\\\";\\n\\nconst basicModel = new ChatOpenAI({ model: \\\"gpt-4o-mini\\\" });\\nconst advancedModel = new ChatOpenAI({ model: \\\"gpt-4o\\\" });\\n\\nconst dynamicModelSelection = createMiddleware({\\n  name: \\\"DynamicModelSelection\\\",\\n  wrapModelCall: (request, handler) => {\\n    // Choose model based on conversation complexity\\n    const messageCount = request.messages.length;\\n\\n    return handler({\\n        ...request,\\n        model: messageCount > 10 ? advancedModel : basicModel,\\n    });\\n  },\\n});\\n\\nconst agent = createAgent({\\n  model: \\\"gpt-4o-mini\\\", // Base model (used when messageCount ≤ 10)\\n  tools,\\n  middleware: [dynamicModelSelection],\\n});\\n\\n\"]\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "### To-do List Middleware\n",
      "\n",
      "1. **Purpose**: The To-do List Middleware is designed to assist agents in planning and tracking complex multi-step tasks. It allows agents to maintain a dynamic to-do list that can be updated as tasks progress.\n",
      "\n",
      "2. **Functionality**: \n",
      "   - The middleware prompts the agent to use a `write_todos` tool to keep track of ongoing tasks.\n",
      "   - It can adapt and update the to-do list in real-time as new information becomes available.\n",
      "\n",
      "3. **Implementation**: \n",
      "   - In Python, you can include it in an agent as follows:\n",
      "     ```python\n",
      "     from langchain.agents import create_agent\n",
      "     from langchain.agents.middleware import TodoListMiddleware\n",
      "\n",
      "     agent = create_agent(\n",
      "         model=\"claude-sonnet-4-5-20250929\",\n",
      "         middleware=[\n",
      "             TodoListMiddleware(\n",
      "                 system_prompt=\"Use the write_todos tool to...\"\n",
      "             ),\n",
      "         ],\n",
      "     )\n",
      "     ```\n",
      "   - In JavaScript, the implementation is similar:\n",
      "     ```javascript\n",
      "     import { createAgent, todoListMiddleware } from \"langchain\";\n",
      "\n",
      "     const agent = createAgent({\n",
      "         model: \"claude-sonnet-4-5-20250929\",\n",
      "         middleware: [\n",
      "             todoListMiddleware({\n",
      "                 systemPrompt: \"Use the write_todos tool to...\",\n",
      "             }),\n",
      "         ],\n",
      "     });\n",
      "     ```\n",
      "\n",
      "4. **Use Cases**: \n",
      "   - Ideal for complex tasks requiring coordination across multiple tools.\n",
      "   - Useful for long-running operations where visibility into progress is important.\n",
      "\n",
      "For more details, you can refer to the [To-do List Middleware documentation](https://docs.langchain.com/oss/python/langchain/middleware/built-in).\n",
      "\n",
      "Now, I will search for information on the provider-specific middleware for Anthropic. \n",
      "\n",
      "Executing search...\n",
      "Tool Calls:\n",
      "  SearchDocsByLangChain (call_tvwh4sDNDHs74dv2EdpzXnrm)\n",
      " Call ID: call_tvwh4sDNDHs74dv2EdpzXnrm\n",
      "  Args:\n",
      "    query: Anthropic middleware\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: SearchDocsByLangChain\n",
      "\n",
      "[\"Title: Provider-specific middleware\\nLink: https://docs.langchain.com/oss/python/integrations/middleware/index\\nContent: Middleware designed for specific providers. Learn more about middleware. Provider Middleware available Anthropic Prompt caching, bash tool, text editor, memory, and file search OpenAI Content moderation Edit the source of this page on GitHub. Connect these docs programmatically to Claude, VSCode, and more via MCP for real-time answers.\\n\\nProvider-specific middleware\\nMiddleware designed for specific providers. Learn more about middleware. Provider Middleware available Anthropic Prompt caching, bash tool, text editor, memory, and file search OpenAI Content moderation Edit the source of this page on GitHub. Connect these docs programmatically to Claude, VSCode, and more via MCP for real-time answers.\\n\\n\", \"Title: Provider-specific middleware\\nLink: https://docs.langchain.com/oss/javascript/integrations/middleware/index\\nContent: Middleware designed for specific providers. Learn more about middleware. Provider Middleware available Anthropic Prompt caching Edit the source of this page on GitHub. Connect these docs programmatically to Claude, VSCode, and more via MCP for real-time answers.\\n\\nProvider-specific middleware\\nMiddleware designed for specific providers. Learn more about middleware. Provider Middleware available Anthropic Prompt caching Edit the source of this page on GitHub. Connect these docs programmatically to Claude, VSCode, and more via MCP for real-time answers.\\n\\n\", \"Title: Provider-specific middleware\\nLink: https://docs.langchain.com/oss/javascript/langchain/middleware/built-in\\nContent: These middleware are optimized for specific LLM providers. See each provider's documentation for full details and examples. Anthropic Prompt caching, bash tool, text editor, memory, and file search middleware for Claude models. Edit the source of this page on GitHub. Connect these docs programmatically to Claude, VSCode, and more via MCP for real-time answers.\\n\\nBuilt-in middleware\\nLangChain provides prebuilt middleware for common use cases. Each middleware is production-ready and configurable for your specific needs.\\n\\nBuilt-in middleware\\nPrebuilt middleware for common agent use cases\\n\\n\", \"Title: Provider-specific middleware\\nLink: https://docs.langchain.com/oss/python/langchain/middleware/built-in\\nContent: These middleware are optimized for specific LLM providers. See each provider's documentation for full details and examples. Anthropic Prompt caching, bash tool, text editor, memory, and file search middleware for Claude models. OpenAI Content moderation middleware for OpenAI models. Edit the source of this page on GitHub. Connect these docs programmatically to Claude, VSCode, and more via MCP for real-time answers.\\n\\nBuilt-in middleware\\nLangChain provides prebuilt middleware for common use cases. Each middleware is production-ready and configurable for your specific needs.\\n\\nBuilt-in middleware\\nPrebuilt middleware for common agent use cases\\n\\n\", \"Title: Anthropic (Claude)\\nLink: https://docs.langchain.com/oss/python/integrations/providers/anthropic\\nContent: This page covers all LangChain integrations with Anthropic , the makers of Claude.\\n\\nAnthropic (Claude)\\nThis page covers all LangChain integrations with Anthropic , the makers of Claude.\\n\\nModel interfaces\\nChatAnthropic Anthropic chat models. Anthropic middleware Anthropic-specific middleware for Claude models.\\n\\n\", \"Title: How to add custom middleware\\nLink: https://docs.langchain.com/langsmith/custom-middleware\\nContent: When deploying agents to LangSmith, you can add custom middleware to your server to handle concerns like logging request metrics, injecting or checking headers, and enforcing security policies without modifying core server logic. This works the same way as adding custom routes . You just need to provide your own Starlette app (including FastAPI , FastHTML and other compatible apps). Adding middleware lets you intercept and modify requests and responses globally across your deployment, whether they're hitting your custom endpoints or the built-in LangSmith APIs. Below is an example using FastAPI. \\\"Python only\\\"\\nWe currently only support custom middleware in Python deployments with langgraph-api>=0.0.26.\\n\\nHow to add custom middleware\\nWhen deploying agents to LangSmith, you can add custom middleware to your server to handle concerns like logging request metrics, injecting or checking headers, and enforcing security policies without modifying core server logic. This works the same way as adding custom routes . You just need to provide your own Starlette app (including FastAPI , FastHTML and other compatible apps). Adding middleware lets you intercept and modify requests and responses globally across your deployment, whether they're hitting your custom endpoints or the built-in LangSmith APIs. Below is an example using FastAPI. \\\"Python only\\\"\\nWe currently only support custom middleware in Python deployments with langgraph-api>=0.0.26.\\n\\nCustomize middleware ordering\\nBy default, custom middleware runs before authentication logic. To run custom middleware after authentication, set middleware_order to auth_first in your http configuration. (This customization is supported starting with API server v0.4.35 and later.)\\n\\n\", \"Title: ChatAnthropic\\nLink: https://docs.langchain.com/oss/javascript/integrations/chat/anthropic\\nContent: Anthropic is an AI safety and research company. They are the creator of Claude. This will help you getting started with Anthropic chat models . For detailed documentation of all ChatAnthropic features and configurations head to the API reference.\\n\\nChatAnthropic\\nAnthropic is an AI safety and research company. They are the creator of Claude. This will help you getting started with Anthropic chat models . For detailed documentation of all ChatAnthropic features and configurations head to the API reference.\\n\\nCustom clients\\nAnthropic models may be hosted on cloud services such as Google Vertex that rely on a different underlying client with the same interface as the primary Anthropic client. You can access these services by providing a createClient method that returns an initialized instance of an Anthropic client. Here's an example:\\n\\n\", \"Title: Trace with Anthropic\\nLink: https://docs.langchain.com/langsmith/trace-anthropic\\nContent: The wrap_anthropic methods in Python allows you to wrap your Anthropic client in order to automatically log traces -- no decorator or function wrapping required! Using the wrapper ensures that messages, including tool calls and multimodal content blocks will be rendered nicely in LangSmith. The wrapper works seamlessly with the @traceable decorator or traceable function and you can use both in the same application. The LANGSMITH_TRACING environment variable must be set to 'true' in order for traces to be logged to LangSmith, even when using wrap_anthropic . This allows you to toggle tracing on and off without changing your code. Additionally, you will need to set the LANGSMITH_API_KEY environment variable to your API key (see Setup for more information). If your LangSmith API key is linked to multiple workspaces, set the LANGSMITH_WORKSPACE_ID environment variable to specify which workspace to use. By default, the traces will be logged to a project named default . To log traces to a different project, see this section. Edit the source of this page on GitHub. Connect these docs programmatically to Claude, VSCode, and more via MCP for real-time answers.\\n\\nThe wrap_anthropic methods in Python allows you to wrap your Anthropic client in order to automatically log traces -- no decorator or function wrapping required! Using the wrapper ensures that messages, including tool calls and multimodal content blocks will be rendered nicely in LangSmith. The wrapper works seamlessly with the @traceable decorator or traceable function and you can use both in the same application. The LANGSMITH_TRACING environment variable must be set to 'true' in order for traces to be logged to LangSmith, even when using wrap_anthropic . This allows you to toggle tracing on and off without changing your code. Additionally, you will need to set the LANGSMITH_API_KEY environment variable to your API key (see Setup for more information). If your LangSmith API key is linked to multiple workspaces, set the LANGSMITH_WORKSPACE_ID environment variable to specify which workspace to use. By default, the traces will be logged to a project named default . To log traces to a different project, see this section. import anthropic\\nfrom langsmith import traceable\\nfrom langsmith.wrappers import wrap_anthropic\\n\\nclient = wrap_anthropic(anthropic.Anthropic())\\n\\n# You can also wrap the async client as well\\n# async_client = wrap_anthropic(anthropic.AsyncAnthropic())\\n\\n@traceable(run_type=\\\"tool\\\", name=\\\"Retrieve Context\\\")\\ndef my_tool(question: str) -> str:\\n    return \\\"During this morning's meeting, we solved all world conflict.\\\"\\n\\n@traceable(name=\\\"Chat Pipeline\\\")\\ndef chat_pipeline(question: str):\\n    context = my_tool(question)\\n    messages = [\\n        { \\\"role\\\": \\\"user\\\", \\\"content\\\": f\\\"Question: {question}\\\\nContext: {context}\\\"}\\n    ]\\n    messages = client.messages.create(\\n      model=\\\"claude-sonnet-4-5-20250929\\\",\\n      messages=messages,\\n      max_tokens=1024,\\n      system=\\\"You are a helpful assistant. Please respond to the user's request only based on the given context.\\\"\\n    )\\n    return messages\\n\\nchat_pipeline(\\\"Can you summarize this morning's meetings?\\\")\\n\\nTrace with Anthropic\\nThe wrap_anthropic methods in Python allows you to wrap your Anthropic client in order to automatically log traces -- no decorator or function wrapping required! Using the wrapper ensures that messages, including tool calls and multimodal content blocks will be rendered nicely in LangSmith. The wrapper works seamlessly with the @traceable decorator or traceable function and you can use both in the same application. The LANGSMITH_TRACING environment variable must be set to 'true' in order for traces to be logged to LangSmith, even when using wrap_anthropic . This allows you to toggle tracing on and off without changing your code. Additionally, you will need to set the LANGSMITH_API_KEY environment variable to your API key (see Setup for more information). If your LangSmith API key is linked to multiple workspaces, set the LANGSMITH_WORKSPACE_ID environment variable to specify which workspace to use. By default, the traces will be logged to a project named default . To log traces to a different project, see this section. import anthropic\\nfrom langsmith import traceable\\nfrom langsmith.wrappers import wrap_anthropic\\n\\nclient = wrap_anthropic(anthropic.Anthropic())\\n\\n# You can also wrap the async client as well\\n# async_client = wrap_anthropic(anthropic.AsyncAnthropic())\\n\\n@traceable(run_type=\\\"tool\\\", name=\\\"Retrieve Context\\\")\\ndef my_tool(question: str) -> str:\\n    return \\\"During this morning's meeting, we solved all world conflict.\\\"\\n\\n@traceable(name=\\\"Chat Pipeline\\\")\\ndef chat_pipeline(question: str):\\n    context = my_tool(question)\\n    messages = [\\n        { \\\"role\\\": \\\"user\\\", \\\"content\\\": f\\\"Question: {question}\\\\nContext: {context}\\\"}\\n    ]\\n    messages = client.messages.create(\\n      model=\\\"claude-sonnet-4-5-20250929\\\",\\n      messages=messages,\\n      max_tokens=1024,\\n      system=\\\"You are a helpful assistant. Please respond to the user's request only based on the given context.\\\"\\n    )\\n    return messages\\n\\nchat_pipeline(\\\"Can you summarize this morning's meetings?\\\")\\n\\n\", \"Title: Custom middleware\\nLink: https://docs.langchain.com/oss/python/langchain/middleware/custom\\nContent: Build custom middleware by implementing hooks that run at specific points in the agent execution flow.\\n\\nWorking with system messages\\nModify system messages in middleware using the system_message field on ModelRequest . The system_message field contains a SystemMessage object (even if the agent was created with a string system_prompt ). Example: Adding context to system message Example: Working with cache control (Anthropic) When working with Anthropic models, you can use structured content blocks with cache control directives to cache large system prompts: from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\\nfrom langchain.messages import SystemMessage\\nfrom typing import Callable\\n\\n\\n@wrap_model_call\\ndef add_cached_context(\\n    request: ModelRequest,\\n    handler: Callable[[ModelRequest], ModelResponse],\\n) -> ModelResponse:\\n    # Always work with content blocks\\n    new_content = list(request.system_message.content_blocks) + [\\n        {\\n            \\\"type\\\": \\\"text\\\",\\n            \\\"text\\\": \\\"Here is a large document to analyze:\\\\n\\\\n<document>...</document>\\\",\\n            # content up until this point is cached\\n            \\\"cache_control\\\": {\\\"type\\\": \\\"ephemeral\\\"}\\n        }\\n    ]\\n\\n    new_system_message = SystemMessage(content=new_content)\\n    return handler(request.override(system_message=new_system_message))\\n\\nWorking with system messages\\nModify system messages in middleware using the system_message field on ModelRequest . The system_message field contains a SystemMessage object (even if the agent was created with a string system_prompt ). Example: Adding context to system message Example: Working with cache control (Anthropic) When working with Anthropic models, you can use structured content blocks with cache control directives to cache large system prompts: from langchain.agents.middleware import AgentMiddleware, ModelRequest, ModelResponse\\nfrom langchain.messages import SystemMessage\\nfrom typing import Callable\\n\\n\\nclass CachedContextMiddleware(AgentMiddleware):\\n    def wrap_model_call(\\n        self,\\n        request: ModelRequest,\\n        handler: Callable[[ModelRequest], ModelResponse],\\n    ) -> ModelResponse:\\n        # Always work with content blocks\\n        new_content = list(request.system_message.content_blocks) + [\\n            {\\n                \\\"type\\\": \\\"text\\\",\\n                \\\"text\\\": \\\"Here is a large document to analyze:\\\\n\\\\n<document>...</document>\\\",\\n                \\\"cache_control\\\": {\\\"type\\\": \\\"ephemeral\\\"}  # This content will be cached\\n            }\\n        ]\\n\\n        new_system_message = SystemMessage(content=new_content)\\n        return handler(request.override(system_message=new_system_message))\\n\\n\", \"Title: Custom middleware\\nLink: https://docs.langchain.com/oss/javascript/langchain/middleware/custom\\nContent: Build custom middleware by implementing hooks that run at specific points in the agent execution flow.\\n\\nCustom middleware\\nBuild custom middleware by implementing hooks that run at specific points in the agent execution flow.\\n\\nWorking with system messages\\nModify system messages in middleware using the systemMessage field in ModelRequest . It contains a SystemMessage object (even if the agent was created with a string systemPrompt ). Example: Chaining middleware - Different middleware can use different approaches: import { createMiddleware, SystemMessage, createAgent } from \\\"langchain\\\";\\n\\n// Middleware 1: Uses systemMessage with simple concatenation\\nconst myMiddleware = createMiddleware({\\n  name: \\\"MyMiddleware\\\",\\n  wrapModelCall: async (request, handler) => {\\n    return handler({\\n      ...request,\\n      systemMessage: request.systemMessage.concat(`Additional context.`),\\n    });\\n  },\\n});\\n\\n// Middleware 2: Uses systemMessage with structured content (preserves structure)\\nconst myOtherMiddleware = createMiddleware({\\n  name: \\\"MyOtherMiddleware\\\",\\n  wrapModelCall: async (request, handler) => {\\n    return handler({\\n      ...request,\\n      systemMessage: request.systemMessage.concat(\\n        new SystemMessage({\\n          content: [\\n            {\\n              type: \\\"text\\\",\\n              text: \\\" More additional context. This will be cached.\\\",\\n              cache_control: { type: \\\"ephemeral\\\", ttl: \\\"5m\\\" },\\n            },\\n          ],\\n        })\\n      ),\\n    });\\n  },\\n});\\n\\nconst agent = createAgent({\\n  model: \\\"anthropic:claude-3-5-sonnet\\\",\\n  systemPrompt: \\\"You are a helpful assistant.\\\",\\n  middleware: [myMiddleware, myOtherMiddleware],\\n});\\n\\n\"]\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "### Provider-Specific Middleware for Anthropic\n",
      "\n",
      "1. **Overview**: LangChain provides middleware specifically designed for the Anthropic models, particularly Claude. This middleware enhances the functionality of agents using Claude by integrating features that are optimized for this provider.\n",
      "\n",
      "2. **Types of Middleware**:\n",
      "   - **Prompt Caching**: This middleware allows for caching of prompts to improve efficiency and reduce latency.\n",
      "   - **Bash Tool**: A middleware that enables the execution of bash commands within the agent's context.\n",
      "   - **Text Editor**: Provides capabilities for text editing tasks.\n",
      "   - **Memory**: Middleware that allows agents to maintain state and remember previous interactions.\n",
      "   - **File Search**: Enables agents to search through files as part of their operations.\n",
      "\n",
      "3. **Implementation**: The middleware can be integrated into agents that utilize Anthropic models. For example, you can create an agent with prompt caching as follows:\n",
      "   ```python\n",
      "   from langchain.agents import create_agent\n",
      "   from langchain.agents.middleware import PromptCachingMiddleware\n",
      "\n",
      "   agent = create_agent(\n",
      "       model=\"claude-sonnet-4-5-20250929\",\n",
      "       middleware=[PromptCachingMiddleware()],\n",
      "   )\n",
      "   ```\n",
      "\n",
      "4. **Use Cases**: \n",
      "   - Ideal for applications that require complex interactions with the Claude model, such as multi-step reasoning or tasks that involve external tools and memory.\n",
      "\n",
      "For more details, you can refer to the [Provider-specific middleware documentation](https://docs.langchain.com/oss/python/integrations/middleware/index).\n",
      "\n",
      "Now, I will search for information on the provider-specific middleware for OpenAI. \n",
      "\n",
      "Executing search...\n",
      "Tool Calls:\n",
      "  SearchDocsByLangChain (call_ai6QZTDMR3M4IylpBPl6dMYv)\n",
      " Call ID: call_ai6QZTDMR3M4IylpBPl6dMYv\n",
      "  Args:\n",
      "    query: OpenAI middleware\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: SearchDocsByLangChain\n",
      "\n",
      "[\"Title: Provider-specific middleware\\nLink: https://docs.langchain.com/oss/python/integrations/middleware/index\\nContent: Middleware designed for specific providers. Learn more about middleware. Provider Middleware available Anthropic Prompt caching, bash tool, text editor, memory, and file search OpenAI Content moderation Edit the source of this page on GitHub. Connect these docs programmatically to Claude, VSCode, and more via MCP for real-time answers.\\n\\nProvider-specific middleware\\nMiddleware designed for specific providers. Learn more about middleware. Provider Middleware available Anthropic Prompt caching, bash tool, text editor, memory, and file search OpenAI Content moderation Edit the source of this page on GitHub. Connect these docs programmatically to Claude, VSCode, and more via MCP for real-time answers.\\n\\n\", \"Title: ChatOpenAI\\nLink: https://docs.langchain.com/oss/javascript/integrations/chat/openai\\nContent: OpenAI is an artificial intelligence (AI) research laboratory. This guide will help you getting started with ChatOpenAI chat models . For detailed documentation of all ChatOpenAI features and configurations head to the API reference. Chat Completions API compatibility ChatOpenAI is fully compatible with OpenAI's (legacy) Chat Completions API . If you are looking to connect to other model providers that support the Chat Completions API, you can do so – see instructions. OpenAI models hosted on Azure Note that certain OpenAI models can also be accessed via the Microsoft Azure platform.\\n\\nChatOpenAI\\nOpenAI is an artificial intelligence (AI) research laboratory. This guide will help you getting started with ChatOpenAI chat models . For detailed documentation of all ChatOpenAI features and configurations head to the API reference. Chat Completions API compatibility ChatOpenAI is fully compatible with OpenAI's (legacy) Chat Completions API . If you are looking to connect to other model providers that support the Chat Completions API, you can do so – see instructions. OpenAI models hosted on Azure Note that certain OpenAI models can also be accessed via the Microsoft Azure platform.\\n\\nResponses API\\nCompatibility The below points apply to @langchain/openai>=0.4.5-rc.0. OpenAI supports a Responses API that is oriented toward building agentic applications. It includes a suite of built-in tools , including web and file search. It also supports management of conversation state , allowing you to continue a conversational thread without explicitly passing in previous messages.  will route to the Responses API if one of these features is used. You can also specify useResponsesApi: true when instantiating ChatOpenAI.\\n\\n\", \"Title: Middleware\\nLink: https://docs.langchain.com/oss/javascript/integrations/providers/openai\\nContent: Middleware specifically designed for OpenAI models. Learn more about middleware. Middleware Description Content moderation Moderate agent traffic using OpenAI's moderation endpoint\\n\\nConfiguration options\\nOpenAI model to use for moderation. Can be either a model name string (e.g.,  \\\"openai:gpt-4o\\\" ) or a  BaseChatModel  instance. The middleware will use this model's client to access the moderation endpoint. OpenAI moderation model to use. Options:  'omni-moderation-latest' ,  'omni-moderation-2024-09-26' ,  'text-moderation-latest' ,  'text-moderation-stable' Whether to check user input messages before the model is called Whether to check model output messages after the model is called Whether to check tool result messages before the model is called How to handle violations when content is flagged. Options: 'end'  - End agent execution immediately with a violation message 'error'  - Throw  OpenAIModerationError  exception 'replace'  - Replace the flagged content with the violation message and continue Custom template for violation messages. Supports template variables: {categories}  - Comma-separated list of flagged categories {category_scores}  - JSON string of category scores {original_content}  - The original flagged content Default:  \\\"I'm sorry, but I can't comply with that request. It was flagged for {categories}.\\\"\\n\\nContent moderation\\nModerate agent traffic (user input, model output, and tool results) using OpenAI's moderation endpoint to detect and handle unsafe content. Content moderation is useful for the following: Applications requiring content safety and compliance Filtering harmful, hateful, or inappropriate content Customer-facing agents that need safety guardrails Meeting platform moderation requirements Learn more about OpenAI's moderation models and categories. API reference: openAIModeration model OpenAI model to use for moderation. Can be either a model name string (e.g., \\\"openai:gpt-4o\\\" ) or a BaseChatModel instance. The middleware will use this model's client to access the moderation endpoint. moderationModel OpenAI moderation model to use. Options: 'omni-moderation-latest' , 'omni-moderation-2024-09-26' , 'text-moderation-latest' , 'text-moderation-stable' checkInput Whether to check user input messages before the model is called checkOutput Whether to check model output messages after the model is called checkToolResults Whether to check tool result messages before the model is called exitBehavior {\\\"body\\\":\\\"exitBehavior\\\",\\\"type\\\":\\\"'error' | 'end' | 'replace'\\\",\\\"default\\\":\\\"'end'\\\"}\\n\\n\", \"Title: AzureOpenAIEmbeddings\\nLink: https://docs.langchain.com/oss/javascript/integrations/text_embedding/azure_openai\\nContent: AzureOpenAIEmbeddings\\nAzure OpenAI is a cloud service to help you quickly develop generative AI experiences with a diverse set of prebuilt and curated models from OpenAI, Meta and beyond. LangChain.js supports integration with Azure OpenAI using the new Azure integration in the OpenAI SDK. You can learn more about Azure OpenAI and its difference with the OpenAI API on this page . If you don't have an Azure account, you can create a free account to get started. This will help you get started with AzureOpenAIEmbeddings embedding models using LangChain. For detailed documentation on AzureOpenAIEmbeddings features and configuration options, please refer to the API reference. Previously, LangChain.js supported integration with Azure OpenAI using the dedicated Azure OpenAI SDK . This SDK is now deprecated in favor of the new Azure integration in the OpenAI SDK, which allows to access the latest OpenAI models and features the same day they are released, and allows seamless transition between the OpenAI API and Azure OpenAI. If you are using Azure OpenAI with the deprecated SDK, see the migration guide to update to the new API.\\n\\nAzure OpenAI is a cloud service to help you quickly develop generative AI experiences with a diverse set of prebuilt and curated models from OpenAI, Meta and beyond. LangChain.js supports integration with Azure OpenAI using the new Azure integration in the OpenAI SDK. You can learn more about Azure OpenAI and its difference with the OpenAI API on this page . If you don't have an Azure account, you can create a free account to get started. This will help you get started with AzureOpenAIEmbeddings embedding models using LangChain. For detailed documentation on AzureOpenAIEmbeddings features and configuration options, please refer to the API reference. Previously, LangChain.js supported integration with Azure OpenAI using the dedicated Azure OpenAI SDK . This SDK is now deprecated in favor of the new Azure integration in the OpenAI SDK, which allows to access the latest OpenAI models and features the same day they are released, and allows seamless transition between the OpenAI API and Azure OpenAI. If you are using Azure OpenAI with the deprecated SDK, see the migration guide to update to the new API.\\n\\nMigration from Azure OpenAI SDK\\nIf you are using the deprecated Azure OpenAI SDK with the @langchain/azure-openai package, you can update your code to use the new Azure integration following these steps: Install the new @langchain/openai package and remove the previous @langchain/azure-openai package: Update your imports to use the new  classe from the @langchain/openai package: import { AzureOpenAIEmbeddings } from \\\"@langchain/openai\\\";\\n\\n\", \"Title: Provider-specific middleware\\nLink: https://docs.langchain.com/oss/python/langchain/middleware/built-in\\nContent: These middleware are optimized for specific LLM providers. See each provider's documentation for full details and examples. Anthropic Prompt caching, bash tool, text editor, memory, and file search middleware for Claude models. OpenAI Content moderation middleware for OpenAI models. Edit the source of this page on GitHub. Connect these docs programmatically to Claude, VSCode, and more via MCP for real-time answers.\\n\\nBuilt-in middleware\\nLangChain provides prebuilt middleware for common use cases. Each middleware is production-ready and configurable for your specific needs.\\n\\nBuilt-in middleware\\nPrebuilt middleware for common agent use cases\\n\\n\", \"Title: Trace with OpenAI Agents SDK\\nLink: https://docs.langchain.com/langsmith/trace-with-openai-agents-sdk\\nContent: The OpenAI Agents SDK allows you to build agentic applications powered by OpenAI's models. Learn how to trace your LLM applications using the OpenAI Agents SDK with LangSmith.\\n\\nTrace with OpenAI Agents SDK\\nThe OpenAI Agents SDK allows you to build agentic applications powered by OpenAI's models. Learn how to trace your LLM applications using the OpenAI Agents SDK with LangSmith.\\n\\nQuick Start\\nYou can integrate LangSmith tracing with the OpenAI Agents SDK by using the OpenAIAgentsTracingProcessor class. The agent's execution flow, including all spans and their details, will be logged to LangSmith. Edit the source of this page on GitHub. Connect these docs programmatically to Claude, VSCode, and more via MCP for real-time answers.\\n\\n\", \"Title: OpenAI\\nLink: https://docs.langchain.com/oss/python/integrations/providers/openai\\nContent: This page covers all LangChain integrations with OpenAI\\n\\nOpenAI\\nThis page covers all LangChain integrations with OpenAI\\n\\nOther\\n (Legacy) OpenAI text completion models. AzureOpenAI Wrapper for (legacy) OpenAI text completion models hosted on Azure. Adapter Adapt LangChain models to OpenAI APIs. OpenAIModerationChain Detect text that could be hateful, violent, etc. Edit the source of this page on GitHub. Connect these docs programmatically to Claude, VSCode, and more via MCP for real-time answers.\\n\\n\", \"Title: Connect to an OpenAI compliant model provider/proxy\\nLink: https://docs.langchain.com/langsmith/custom-openai-compliant-model\\nContent: The LangSmith playground allows you to use any model that is compliant with the OpenAI API. You can utilize your model by setting the Proxy Provider for  in the playground.\\n\\nConnect to an OpenAI compliant model provider/proxy\\nThe LangSmith playground allows you to use any model that is compliant with the OpenAI API. You can utilize your model by setting the Proxy Provider for  in the playground.\\n\\nDeploy an OpenAI compliant model\\nMany providers offer OpenAI compliant models or proxy services. Some examples of this include: LiteLLM Proxy Ollama You can use these providers to deploy your model and get an API endpoint that is compliant with the OpenAI API. Take a look at the full specification for more information.\\n\\n\", \"Title: Nov 25, 2025\\nLink: https://docs.langchain.com/oss/javascript/releases/changelog\\nContent: v1.1.0 Model profiles : Chat models now expose supported features and capabilities through a  .profile  getter. These data are derived from  models.dev , an open source project providing model capability data. Model retry middleware : New middleware for automatically retrying failed model calls with configurable exponential backoff, improving agent reliability. Content moderation middleware : OpenAI content moderation middleware for detecting and handling unsafe content in agent interactions. Supports checking user input, model output, and tool results. Summarization middleware : Updated to support flexible trigger points using model profiles for context-aware summarization. Structured output :  ProviderStrategy  support (native structured output) can now be inferred from model profiles. SystemMessage  for  createAgent : Support for passing  SystemMessage  instances directly to  createAgent 's  systemPrompt  parameter and a new  concat  method for extending system messages. Enables advanced features like cache control and structured content blocks. Dynamic system prompt middleware : Return values from  dynamicSystemPromptMiddleware  are now purely additive. When returning a  SystemMessage  or  string , they are merged with existing system messages rather than replacing them, making it easier to compose multiple middleware that modify the prompt. Compatibility improvements:  Fixed error handling for Zod v4 validation errors in structured output and tool schemas, ensuring detailed error messages are properly displayed.\\n\\nv1.1.0\\nModel profiles : Chat models now expose supported features and capabilities through a .profile getter. These data are derived from models.dev , an open source project providing model capability data. Model retry middleware : New middleware for automatically retrying failed model calls with configurable exponential backoff, improving agent reliability. Content moderation middleware : OpenAI content moderation middleware for detecting and handling unsafe content in agent interactions. Supports checking user input, model output, and tool results. Summarization middleware : Updated to support flexible trigger points using model profiles for context-aware summarization. Structured output : ProviderStrategy support (native structured output) can now be inferred from model profiles. SystemMessage for createAgent : Support for passing SystemMessage instances directly to createAgent 's systemPrompt parameter and a new concat method for extending system messages. Enables advanced features like cache control and structured content blocks. Dynamic system prompt middleware : Return values from dynamicSystemPromptMiddleware are now purely additive. When returning a SystemMessage or string , they are merged with existing system messages rather than replacing them, making it easier to compose multiple middleware that modify the prompt. Compatibility improvements: Fixed error handling for Zod v4 validation errors in structured output and tool schemas, ensuring detailed error messages are properly displayed.\\n\\nOct 20, 2025\\nv1.0.0 langchain Release notes Migration guide langgraph Release notes Migration guide If you encounter any issues or have feedback, please  open an issue  so we can improve. To view v0.x documentation,  go to the archived content .\\n\\n\", \"Title: How to add custom middleware\\nLink: https://docs.langchain.com/langsmith/custom-middleware\\nContent: When deploying agents to LangSmith, you can add custom middleware to your server to handle concerns like logging request metrics, injecting or checking headers, and enforcing security policies without modifying core server logic. This works the same way as adding custom routes . You just need to provide your own Starlette app (including FastAPI , FastHTML and other compatible apps). Adding middleware lets you intercept and modify requests and responses globally across your deployment, whether they're hitting your custom endpoints or the built-in LangSmith APIs. Below is an example using FastAPI. \\\"Python only\\\"\\nWe currently only support custom middleware in Python deployments with langgraph-api>=0.0.26.\\n\\nHow to add custom middleware\\nWhen deploying agents to LangSmith, you can add custom middleware to your server to handle concerns like logging request metrics, injecting or checking headers, and enforcing security policies without modifying core server logic. This works the same way as adding custom routes . You just need to provide your own Starlette app (including FastAPI , FastHTML and other compatible apps). Adding middleware lets you intercept and modify requests and responses globally across your deployment, whether they're hitting your custom endpoints or the built-in LangSmith APIs. Below is an example using FastAPI. \\\"Python only\\\"\\nWe currently only support custom middleware in Python deployments with langgraph-api>=0.0.26.\\n\\nCustomize middleware ordering\\nBy default, custom middleware runs before authentication logic. To run custom middleware after authentication, set middleware_order to auth_first in your http configuration. (This customization is supported starting with API server v0.4.35 and later.)\\n\\n\"]\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "### Provider-Specific Middleware for OpenAI\n",
      "\n",
      "1. **Overview**: LangChain provides middleware specifically designed for OpenAI models, enhancing the functionality of agents that utilize these models.\n",
      "\n",
      "2. **Types of Middleware**:\n",
      "   - **Content Moderation**: This middleware is used to moderate agent traffic by checking user input, model output, and tool results against OpenAI's moderation endpoint. It helps in detecting and handling unsafe content.\n",
      "   - **Configuration Options**: \n",
      "     - You can specify which OpenAI model to use for moderation.\n",
      "     - Options for handling flagged content include ending execution, throwing an error, or replacing flagged content with a violation message.\n",
      "\n",
      "3. **Implementation**: You can implement content moderation middleware in an agent as follows:\n",
      "   ```python\n",
      "   from langchain.agents import create_agent\n",
      "   from langchain.agents.middleware import OpenAIModerationMiddleware\n",
      "\n",
      "   agent = create_agent(\n",
      "       model=\"openai:gpt-4o\",\n",
      "       middleware=[OpenAIModerationMiddleware()],\n",
      "   )\n",
      "   ```\n",
      "\n",
      "4. **Use Cases**: \n",
      "   - Essential for applications that require content safety and compliance, such as customer-facing agents or platforms that need to meet moderation requirements.\n",
      "\n",
      "For more details, you can refer to the [Provider-specific middleware documentation](https://docs.langchain.com/oss/python/integrations/middleware/index).\n",
      "\n",
      "Now, I will proceed to Step 4, where I will search for limitations, failure modes, or \"gotchas\" associated with these middlewares. \n",
      "\n",
      "Executing search...\n",
      "Tool Calls:\n",
      "  SearchDocsByLangChain (call_TuIOsNuqHT9WbvReR7iUrXZP)\n",
      " Call ID: call_TuIOsNuqHT9WbvReR7iUrXZP\n",
      "  Args:\n",
      "    query: limitations of LangChain middleware\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: SearchDocsByLangChain\n",
      "\n",
      "[\"Title: Built-in middleware\\nLink: https://docs.langchain.com/oss/python/langchain/middleware/built-in\\nContent: LangChain provides prebuilt middleware for common use cases. Each middleware is production-ready and configurable for your specific needs.\\n\\nShell tool\\nExpose a persistent shell session to agents for command execution. Shell tool middleware is useful for the following: Agents that need to execute system commands Development and deployment automation tasks Testing and validation workflows File system operations and script execution Security consideration : Use appropriate execution policies ( HostExecutionPolicy , DockerExecutionPolicy , or CodexSandboxExecutionPolicy ) to match your deployment's security requirements. Limitation : Persistent shell sessions do not currently work with interrupts (human-in-the-loop). We anticipate adding support for this in the future. API reference: ShellToolMiddleware from langchain.agents import create_agent\\nfrom langchain.agents.middleware import (\\n    ShellToolMiddleware,\\n    HostExecutionPolicy,\\n)\\n\\nagent = create_agent(\\n    model=\\\"gpt-4o\\\",\\n    tools=[search_tool],\\n    middleware=[\\n        ShellToolMiddleware(\\n            workspace_root=\\\"/workspace\\\",\\n            execution_policy=HostExecutionPolicy(),\\n        ),\\n    ],\\n)\\n\\nTool call limit\\nControl agent execution by limiting the number of tool calls, either globally across all tools or for specific tools. Tool call limits are useful for the following: Preventing excessive calls to expensive external APIs. Limiting web searches or database queries. Enforcing rate limits on specific tool usage. Protecting against runaway agent loops. API reference: ToolCallLimitMiddleware from langchain.agents import create_agent\\nfrom langchain.agents.middleware import ToolCallLimitMiddleware\\n\\nagent = create_agent(\\n    model=\\\"gpt-4o\\\",\\n    tools=[search_tool, database_tool],\\n    middleware=[\\n        # Global limit\\n        ToolCallLimitMiddleware(thread_limit=20, run_limit=10),\\n        # Tool-specific limit\\n        ToolCallLimitMiddleware(\\n            tool_name=\\\"search\\\",\\n            thread_limit=5,\\n            run_limit=3,\\n        ),\\n    ],\\n)\\n\\n\", \"Title: Built-in middleware\\nLink: https://docs.langchain.com/oss/javascript/langchain/middleware/built-in\\nContent: LangChain provides prebuilt middleware for common use cases. Each middleware is production-ready and configurable for your specific needs.\\n\\nTool call limit\\nControl agent execution by limiting the number of tool calls, either globally across all tools or for specific tools. Tool call limits are useful for the following: Preventing excessive calls to expensive external APIs. Limiting web searches or database queries. Enforcing rate limits on specific tool usage. Protecting against runaway agent loops. import { createAgent, toolCallLimitMiddleware } from \\\"langchain\\\";\\n\\nconst agent = createAgent({\\n  model: \\\"gpt-4o\\\",\\n  tools: [searchTool, databaseTool],\\n  middleware: [\\n    toolCallLimitMiddleware({ threadLimit: 20, runLimit: 10 }),\\n    toolCallLimitMiddleware({\\n      toolName: \\\"search\\\",\\n      threadLimit: 5,\\n      runLimit: 3,\\n    }),\\n  ],\\n});\\n\\nModel call limit\\nLimit the number of model calls to prevent infinite loops or excessive costs. Model call limit is useful for the following: Preventing runaway agents from making too many API calls. Enforcing cost controls on production deployments. Testing agent behavior within specific call budgets. import { createAgent, modelCallLimitMiddleware } from \\\"langchain\\\";\\n\\nconst agent = createAgent({\\n  model: \\\"gpt-4o\\\",\\n  tools: [],\\n  middleware: [\\n    modelCallLimitMiddleware({\\n      threadLimit: 10,\\n      runLimit: 5,\\n      exitBehavior: \\\"end\\\",\\n    }),\\n  ],\\n});\\n\\n\", \"Title: LangChain overview\\nLink: https://docs.langchain.com/oss/javascript/langchain/overview\\nContent: LangChain is the easiest way to start building agents and applications powered by LLMs. With under 10 lines of code, you can connect to OpenAI, Anthropic, Google, and more . LangChain provides a pre-built agent architecture and model integrations to help you get started quickly and seamlessly incorporate LLMs into your agents and applications. We recommend you use LangChain if you want to quickly build agents and autonomous applications. Use LangGraph , our low-level agent orchestration framework and runtime, when you have more advanced needs that require a combination of deterministic and agentic workflows, heavy customization, and carefully controlled latency. LangChain agents are built on top of LangGraph in order to provide durable execution, streaming, human-in-the-loop, persistence, and more. You do not need to know LangGraph for basic LangChain agent usage.\\n\\nLangChain overview\\nLangChain is the easiest way to start building agents and applications powered by LLMs. With under 10 lines of code, you can connect to OpenAI, Anthropic, Google, and more . LangChain provides a pre-built agent architecture and model integrations to help you get started quickly and seamlessly incorporate LLMs into your agents and applications. We recommend you use LangChain if you want to quickly build agents and autonomous applications. Use LangGraph , our low-level agent orchestration framework and runtime, when you have more advanced needs that require a combination of deterministic and agentic workflows, heavy customization, and carefully controlled latency. LangChain agents are built on top of LangGraph in order to provide durable execution, streaming, human-in-the-loop, persistence, and more. You do not need to know LangGraph for basic LangChain agent usage.\\n\\nCore benefits\\nStandard model interface Different providers have unique APIs for interacting with models, including the format of responses. LangChain standardizes how you interact with models so that you can seamlessly swap providers and avoid lock-in. Easy to use, highly flexible agent LangChain's agent abstraction is designed to be easy to get started with, letting you build a simple agent in under 10 lines of code. But it also provides enough flexibility to allow you to do all the context engineering your heart desires. Built on top of LangGraph LangChain's agents are built on top of LangGraph. This allows us to take advantage of LangGraph's durable execution, human-in-the-loop support, persistence, and more. Debug with LangSmith Gain deep visibility into complex agent behavior with visualization tools that trace execution paths, capture state transitions, and provide detailed runtime metrics. Edit the source of this page on GitHub. Connect these docs programmatically to Claude, VSCode, and more via MCP for real-time answers.\\n\\n\", \"Title: LangChain overview\\nLink: https://docs.langchain.com/oss/python/langchain/overview\\nContent: LangChain is the easiest way to start building agents and applications powered by LLMs. With under 10 lines of code, you can connect to OpenAI, Anthropic, Google, and more . LangChain provides a pre-built agent architecture and model integrations to help you get started quickly and seamlessly incorporate LLMs into your agents and applications. We recommend you use LangChain if you want to quickly build agents and autonomous applications. Use LangGraph , our low-level agent orchestration framework and runtime, when you have more advanced needs that require a combination of deterministic and agentic workflows, heavy customization, and carefully controlled latency. LangChain agents are built on top of LangGraph in order to provide durable execution, streaming, human-in-the-loop, persistence, and more. You do not need to know LangGraph for basic LangChain agent usage.\\n\\nLangChain overview\\nLangChain is the easiest way to start building agents and applications powered by LLMs. With under 10 lines of code, you can connect to OpenAI, Anthropic, Google, and more . LangChain provides a pre-built agent architecture and model integrations to help you get started quickly and seamlessly incorporate LLMs into your agents and applications. We recommend you use LangChain if you want to quickly build agents and autonomous applications. Use LangGraph , our low-level agent orchestration framework and runtime, when you have more advanced needs that require a combination of deterministic and agentic workflows, heavy customization, and carefully controlled latency. LangChain agents are built on top of LangGraph in order to provide durable execution, streaming, human-in-the-loop, persistence, and more. You do not need to know LangGraph for basic LangChain agent usage.\\n\\nCore benefits\\nStandard model interface Different providers have unique APIs for interacting with models, including the format of responses. LangChain standardizes how you interact with models so that you can seamlessly swap providers and avoid lock-in. Easy to use, highly flexible agent LangChain's agent abstraction is designed to be easy to get started with, letting you build a simple agent in under 10 lines of code. But it also provides enough flexibility to allow you to do all the context engineering your heart desires. Built on top of LangGraph LangChain's agents are built on top of LangGraph. This allows us to take advantage of LangGraph's durable execution, human-in-the-loop support, persistence, and more. Debug with LangSmith Gain deep visibility into complex agent behavior with visualization tools that trace execution paths, capture state transitions, and provide detailed runtime metrics. Edit the source of this page on GitHub. Connect these docs programmatically to Claude, VSCode, and more via MCP for real-time answers.\\n\\n\", \"Title: Prebuilt middleware\\nLink: https://docs.langchain.com/oss/python/releases/langchain-v1\\nContent: LangChain provides a few prebuilt middlewares for common patterns, including: PIIMiddleware : Redact sensitive information before sending to the model SummarizationMiddleware : Condense conversation history when it gets too long HumanInTheLoopMiddleware : Require approval for sensitive tool calls\\n\\nWhat's new in LangChain v1\\nLangChain v1 is a focused, production-ready foundation for building agents. We've streamlined the framework around three core improvements: create_agent The new standard for building agents in LangChain, replacing langgraph.prebuilt.create_react_agent. Standard content blocks A new content_blocks property that provides unified access to modern LLM features across providers. Simplified namespace The langchain namespace has been streamlined to focus on essential building blocks for agents, with legacy functionality moved to langchain-classic. To upgrade, uv add langchain\\n\\nWhat's new in LangChain v1\\nLangChain v1 is a focused, production-ready foundation for building agents. We've streamlined the framework around three core improvements: create_agent The new standard for building agents in LangChain, replacing langgraph.prebuilt.create_react_agent. Standard content blocks A new content_blocks property that provides unified access to modern LLM features across providers. Simplified namespace The langchain namespace has been streamlined to focus on essential building blocks for agents, with legacy functionality moved to langchain-classic. To upgrade, For a complete list of changes, see the migration guide.\\n\\n\", \"Title: Prebuilt middleware\\nLink: https://docs.langchain.com/oss/javascript/releases/langchain-v1\\nContent: LangChain provides a few prebuilt middlewares for common patterns, including: summarizationMiddleware : Condense conversation history when it gets too long humanInTheLoopMiddleware : Require approval for sensitive tool calls piiRedactionMiddleware : Redact sensitive information before sending to the model\\n\\nWhat's new in LangChain v1\\nLangChain v1 is a focused, production-ready foundation for building agents. We've streamlined the framework around three core improvements: createAgent A new standard way to build agents in LangChain, replacing createReactAgent from LangGraph with a cleaner, more powerful API. Standard content blocks A new contentBlocks property that provides unified access to modern LLM features across all providers. Simplified package The langchain package has been streamlined to focus on essential building blocks for agents, with legacy functionality moved to @langchain/classic. To upgrade, For a complete list of changes, see the migration guide.\\n\\nWhat's new in LangChain v1\\nLangChain v1 is a focused, production-ready foundation for building agents. We've streamlined the framework around three core improvements: createAgent A new standard way to build agents in LangChain, replacing createReactAgent from LangGraph with a cleaner, more powerful API. Standard content blocks A new contentBlocks property that provides unified access to modern LLM features across all providers. Simplified package The langchain package has been streamlined to focus on essential building blocks for agents, with legacy functionality moved to @langchain/classic. To upgrade, yarn add langchain @langchain/core\\n\\n\", \"Title: undefined\\nLink: https://docs.langchain.com/\\nContent: \", \"Title: How to add custom middleware\\nLink: https://docs.langchain.com/langsmith/custom-middleware\\nContent: When deploying agents to LangSmith, you can add custom middleware to your server to handle concerns like logging request metrics, injecting or checking headers, and enforcing security policies without modifying core server logic. This works the same way as adding custom routes . You just need to provide your own Starlette app (including FastAPI , FastHTML and other compatible apps). Adding middleware lets you intercept and modify requests and responses globally across your deployment, whether they're hitting your custom endpoints or the built-in LangSmith APIs. Below is an example using FastAPI. \\\"Python only\\\"\\nWe currently only support custom middleware in Python deployments with langgraph-api>=0.0.26.\\n\\nHow to add custom middleware\\nWhen deploying agents to LangSmith, you can add custom middleware to your server to handle concerns like logging request metrics, injecting or checking headers, and enforcing security policies without modifying core server logic. This works the same way as adding custom routes . You just need to provide your own Starlette app (including FastAPI , FastHTML and other compatible apps). Adding middleware lets you intercept and modify requests and responses globally across your deployment, whether they're hitting your custom endpoints or the built-in LangSmith APIs. Below is an example using FastAPI. \\\"Python only\\\"\\nWe currently only support custom middleware in Python deployments with langgraph-api>=0.0.26.\\n\\nCreate app\\nStarting from an existing LangSmith application, add the following middleware code to your webapp.py file. If you are starting from scratch, you can create a new app from a template using the CLI. Once you have a LangGraph project, add the following app code:\\n\\n\", \"Title: How it works\\nLink: https://docs.langchain.com/oss/javascript/langchain/context-engineering\\nContent: LangChain middleware is the mechanism under the hood that makes context engineering practical for developers using LangChain. Middleware allows you to hook into any step in the agent lifecycle and: Update context Jump to a different step in the agent lifecycle Throughout this guide, you'll see frequent use of the middleware API as a means to the context engineering end.\\n\\nExample: Summarization\\nOne of the most common life-cycle patterns is automatically condensing conversation history when it gets too long. Unlike the transient message trimming shown in Model Context , summarization persistently updates state - permanently replacing old messages with a summary that's saved for all future turns. LangChain offers built-in middleware for this: When the conversation exceeds the token limit, SummarizationMiddleware automatically: Summarizes older messages using a separate LLM call Replaces them with a summary message in State (permanently) Keeps recent messages intact for context The summarized conversation history is permanently updated - future turns will see the summary instead of the original messages. For a complete list of built-in middleware, available hooks, and how to create custom middleware, see the Middleware documentation.\\n\\nExample: Summarization\\nOne of the most common life-cycle patterns is automatically condensing conversation history when it gets too long. Unlike the transient message trimming shown in Model Context , summarization persistently updates state - permanently replacing old messages with a summary that's saved for all future turns. LangChain offers built-in middleware for this: import { createAgent, summarizationMiddleware } from \\\"langchain\\\";\\n\\nconst agent = createAgent({\\n  model: \\\"gpt-4o\\\",\\n  tools: [...],\\n  middleware: [\\n    summarizationMiddleware({\\n      model: \\\"gpt-4o-mini\\\",\\n      trigger: { tokens: 4000 },\\n      keep: { messages: 20 },\\n    }),\\n  ],\\n});\\n\\n\", \"Title: How it works\\nLink: https://docs.langchain.com/oss/python/langchain/context-engineering\\nContent: LangChain middleware is the mechanism under the hood that makes context engineering practical for developers using LangChain. Middleware allows you to hook into any step in the agent lifecycle and: Update context Jump to a different step in the agent lifecycle Throughout this guide, you'll see frequent use of the middleware API as a means to the context engineering end.\\n\\nExample: Summarization\\nOne of the most common life-cycle patterns is automatically condensing conversation history when it gets too long. Unlike the transient message trimming shown in Model Context , summarization persistently updates state - permanently replacing old messages with a summary that's saved for all future turns. LangChain offers built-in middleware for this: When the conversation exceeds the token limit, SummarizationMiddleware automatically: Summarizes older messages using a separate LLM call Replaces them with a summary message in State (permanently) Keeps recent messages intact for context The summarized conversation history is permanently updated - future turns will see the summary instead of the original messages. For a complete list of built-in middleware, available hooks, and how to create custom middleware, see the Middleware documentation.\\n\\nExample: Summarization\\nOne of the most common life-cycle patterns is automatically condensing conversation history when it gets too long. Unlike the transient message trimming shown in Model Context , summarization persistently updates state - permanently replacing old messages with a summary that's saved for all future turns. LangChain offers built-in middleware for this: from langchain.agents import create_agent\\nfrom langchain.agents.middleware import SummarizationMiddleware\\n\\nagent = create_agent(\\n    model=\\\"gpt-4o\\\",\\n    tools=[...],\\n    middleware=[\\n        SummarizationMiddleware(\\n            model=\\\"gpt-4o-mini\\\",\\n            trigger={\\\"tokens\\\": 4000},\\n            keep={\\\"messages\\\": 20},\\n        ),\\n    ],\\n)\\n\\n\"]\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "### Limitations and Gotchas of LangChain Middleware\n",
      "\n",
      "1. **General Limitations**:\n",
      "   - Middleware is designed to be production-ready, but specific configurations may be required to meet security and operational needs.\n",
      "   - For example, the **Shell Tool Middleware** has a limitation where persistent shell sessions do not currently support interrupts (human-in-the-loop). This feature is anticipated to be added in future updates.\n",
      "\n",
      "2. **Tool Call Limits**:\n",
      "   - Middleware can enforce limits on the number of tool calls to prevent excessive usage, which is useful for controlling costs and preventing runaway agent loops. However, if not configured properly, it may lead to unintended restrictions on agent capabilities.\n",
      "\n",
      "3. **Model Call Limits**:\n",
      "   - Similar to tool call limits, model call limits can prevent infinite loops or excessive costs. If the limits are set too low, it may hinder the agent's ability to perform complex tasks that require multiple calls.\n",
      "\n",
      "4. **Content Moderation**:\n",
      "   - The **Content Moderation Middleware** for OpenAI models can be strict, potentially leading to false positives where legitimate content is flagged. This can disrupt user experience if not handled gracefully.\n",
      "\n",
      "5. **Dynamic Behavior**:\n",
      "   - Middleware that modifies system prompts or manages state can introduce complexity. For instance, if multiple middleware components are modifying the same state or prompt, it may lead to unpredictable behavior unless carefully managed.\n",
      "\n",
      "For more detailed information, you can refer to the [Built-in middleware documentation](https://docs.langchain.com/oss/python/langchain/middleware/built-in).\n",
      "\n",
      "### Step 5: Final Consolidated Answer\n",
      "\n",
      "LangChain provides a robust set of built-in middleware designed to enhance the functionality of agents powered by large language models (LLMs). Here’s a comprehensive overview:\n",
      "\n",
      "1. **Types of Built-in Middleware**:\n",
      "   - **To-do List Middleware**: Assists agents in planning and tracking complex multi-step tasks by maintaining a dynamic to-do list.\n",
      "   - **Provider-Specific Middleware**:\n",
      "     - **Anthropic Middleware**: Includes prompt caching, bash tool, text editor, memory, and file search functionalities tailored for Claude models.\n",
      "     - **OpenAI Middleware**: Primarily focuses on content moderation to ensure safe interactions by checking user inputs and model outputs.\n",
      "\n",
      "2. **Implementation**: Middleware can be easily integrated into agents using simple code snippets in both Python and JavaScript, allowing for customization based on specific needs.\n",
      "\n",
      "3. **Use Cases**: Middleware is beneficial for applications requiring task management, content safety, and complex interactions with LLMs.\n",
      "\n",
      "4. **Limitations**: While middleware is powerful, it comes with certain limitations, such as potential false positives in content moderation, restrictions on tool and model calls, and complexities arising from dynamic behavior modifications.\n",
      "\n",
      "5. **Resources**: For further details, you can explore the [LangChain documentation](https://docs.langchain.com/oss/python/langchain/middleware/built-in) and specific sections on provider-specific middleware.\n",
      "\n",
      "This overview should provide a solid understanding of LangChain's built-in middleware and its implications for developing LLM-powered applications.\n"
     ]
    }
   ],
   "source": [
    "for m in response['messages']:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at that message count. The agent went on quite a journey.\n",
    "\n",
    "In a real app, this could mean:\n",
    "- **Cost explosion** – each model call costs money\n",
    "- **Latency nightmare** – users waiting forever\n",
    "- **Infinite loops** – if something goes wrong, the agent might never stop\n",
    "\n",
    "Time to put some boundaries in place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Adding a Leash: ModelCallLimitMiddleware\n",
    "\n",
    "The middleware is simple. You tell it the maximum number of model calls allowed, and it enforces that limit.\n",
    "\n",
    "Two key parameters:\n",
    "- `run_limit` – max calls for a single `.invoke()` \n",
    "- `exit_behavior` – what happens when the limit is hit: `\"end\"` (graceful stop) or `\"error\"` (raise an exception)\n",
    "\n",
    "Let's cap our agent at 3 model calls and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/png;base64,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\" style=\"display:block;margin:0 auto;width:100%;max-width:min(90vw, 300px);max-height:70vh;height:auto\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "limited_agent = create_agent(\n",
    "    model=MODEL,\n",
    "    tools=LANGCHAIN_DOCS_TOOLS,\n",
    "    system_prompt=SYSTEM_PROMPT,\n",
    "    checkpointer=InMemorySaver(),\n",
    "    middleware=[\n",
    "        ModelCallLimitMiddleware(\n",
    "            run_limit=3,\n",
    "            exit_behavior=\"end\",\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "\n",
    "draw_mermaid_png(limited_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total messages in conversation: 8\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": str(uuid.uuid4())}}\n",
    "\n",
    "response = await limited_agent.ainvoke(\n",
    "    {\"messages\": [HumanMessage(RESEARCH_TASK)]},\n",
    "    config=config\n",
    ")\n",
    "\n",
    "print(f\"Total messages in conversation: {len(response['messages'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "\n",
      "You are researching LangChain's built-in middleware.\n",
      "Work in at least 5 clearly separated AI steps. After each step, send an answer\n",
      "before continuing.\n",
      "\n",
      "1. Step 1: Plan your research on built-in middleware (no tool calls yet). Output only the plan.\n",
      "2. Step 2: Use the docs search tool to find a complete list of built-in middlewares. Output what you found.\n",
      "3. Step 3: For each middleware in that list, run a separate docs search focused on that middleware and update your notes.\n",
      "4. Step 4: Run another docs search focused only on limitations / failure modes / “gotchas” for these middlewares. Update your notes.\n",
      "5. Step 5: Only after all previous steps, write the final consolidated answer for me.\n",
      "\n",
      "At the end of each step, explicitly write: `STEP X DONE`, then continue with the next step.\n",
      "\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "### Step 1: Plan your research on built-in middleware\n",
      "\n",
      "1. **Define Middleware**: Understand what middleware means in the context of LangChain and its role in the architecture.\n",
      "2. **Identify Built-in Middleware**: Search for a comprehensive list of built-in middleware provided by LangChain.\n",
      "3. **Examine Each Middleware**: For each middleware identified, gather detailed information about its functionality, use cases, and examples.\n",
      "4. **Investigate Limitations**: Look for any documented limitations, failure modes, or common issues associated with the built-in middleware.\n",
      "5. **Consolidate Findings**: Compile all the gathered information into a coherent summary that highlights the key aspects of LangChain's built-in middleware.\n",
      "\n",
      "STEP 1 DONE. \n",
      "\n",
      "### Step 2: Use the docs search tool to find a complete list of built-in middlewares\n",
      "\n",
      "Now, I will search the LangChain documentation for a complete list of built-in middleware.\n",
      "Tool Calls:\n",
      "  SearchDocsByLangChain (call_TPBiTAz58XajM0BVMNdvMSJF)\n",
      " Call ID: call_TPBiTAz58XajM0BVMNdvMSJF\n",
      "  Args:\n",
      "    query: built-in middleware\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: SearchDocsByLangChain\n",
      "\n",
      "[\"Title: Built-in middleware\\nLink: https://docs.langchain.com/oss/javascript/langchain/middleware/built-in\\nContent: LangChain provides prebuilt middleware for common use cases. Each middleware is production-ready and configurable for your specific needs.\\n\\nPrebuilt middleware for common agent use cases\\n\\nProvider-specific middleware\\nThese middleware are optimized for specific LLM providers. See each provider's documentation for full details and examples. Anthropic Prompt caching, bash tool, text editor, memory, and file search middleware for Claude models. Edit the source of this page on GitHub. Connect these docs programmatically to Claude, VSCode, and more via MCP for real-time answers.\\n\\n\", \"Title: Built-in middleware\\nLink: https://docs.langchain.com/oss/python/langchain/middleware/built-in\\nContent: LangChain provides prebuilt middleware for common use cases. Each middleware is production-ready and configurable for your specific needs.\\n\\nPrebuilt middleware for common agent use cases\\n\\nTo-do list\\nEquip agents with task planning and tracking capabilities for complex multi-step tasks. To-do lists are useful for the following: Complex multi-step tasks requiring coordination across multiple tools. Long-running operations where progress visibility is important. This middleware automatically provides agents with a write_todos tool and system prompts to guide effective task planning. API reference: TodoListMiddleware Watch this video guide demonstrating To-do List middleware behavior. system_prompt Custom system prompt for guiding todo usage. Uses built-in prompt if not specified. tool_description Custom description for the write_todos tool. Uses built-in description if not specified.\\n\\n\", \"Title: How to add custom middleware\\nLink: https://docs.langchain.com/langsmith/custom-middleware\\nContent: When deploying agents to LangSmith, you can add custom middleware to your server to handle concerns like logging request metrics, injecting or checking headers, and enforcing security policies without modifying core server logic. This works the same way as adding custom routes . You just need to provide your own Starlette app (including FastAPI , FastHTML and other compatible apps). Adding middleware lets you intercept and modify requests and responses globally across your deployment, whether they're hitting your custom endpoints or the built-in LangSmith APIs. Below is an example using FastAPI. \\\"Python only\\\"\\nWe currently only support custom middleware in Python deployments with langgraph-api>=0.0.26.\\n\\nHow to add custom middleware\\nWhen deploying agents to LangSmith, you can add custom middleware to your server to handle concerns like logging request metrics, injecting or checking headers, and enforcing security policies without modifying core server logic. This works the same way as adding custom routes . You just need to provide your own Starlette app (including FastAPI , FastHTML and other compatible apps). Adding middleware lets you intercept and modify requests and responses globally across your deployment, whether they're hitting your custom endpoints or the built-in LangSmith APIs. Below is an example using FastAPI. \\\"Python only\\\"\\nWe currently only support custom middleware in Python deployments with langgraph-api>=0.0.26.\\n\\nCustomize middleware ordering\\nBy default, custom middleware runs before authentication logic. To run custom middleware after authentication, set middleware_order to auth_first in your http configuration. (This customization is supported starting with API server v0.4.35 and later.)\\n\\n\", \"Title: Custom middleware\\nLink: https://docs.langchain.com/oss/python/langchain/middleware/custom\\nContent: Build custom middleware by implementing hooks that run at specific points in the agent execution flow.\\n\\nCustom middleware\\nBuild custom middleware by implementing hooks that run at specific points in the agent execution flow.\\n\\nCreate middleware\\nYou can create middleware in two ways: Decorator-based middleware Quick and simple for single-hook middleware. Use decorators to wrap individual functions. Class-based middleware More powerful for complex middleware with multiple hooks or configuration.\\n\\n\", \"Title: Custom middleware\\nLink: https://docs.langchain.com/oss/javascript/langchain/middleware/custom\\nContent: Build custom middleware by implementing hooks that run at specific points in the agent execution flow.\\n\\nCustom middleware\\nBuild custom middleware by implementing hooks that run at specific points in the agent execution flow.\\n\\nAdditional resources\\nMiddleware API reference Built-in middleware Testing agents Edit the source of this page on GitHub. Connect these docs programmatically to Claude, VSCode, and more via MCP for real-time answers.\\n\\n\", \"Title: Deep Agents Middleware\\nLink: https://docs.langchain.com/oss/javascript/deepagents/middleware\\nContent: Deep agents are built with a modular middleware architecture. Deep agents have access to: A planning tool A filesystem for storing context and long-term memories The ability to spawn subagents Each feature is implemented as separate middleware. When you create a deep agent with create_deep_agent , we automatically attach TodoListMiddleware , FilesystemMiddleware , and SubAgentMiddleware to your agent. Middleware is composable—you can add as many or as few middleware to an agent as needed. You can use any middleware independently. The following sections explain what each middleware provides.\\n\\nDeep agents are built with a modular middleware architecture. Deep agents have access to: A planning tool A filesystem for storing context and long-term memories The ability to spawn subagents Each feature is implemented as separate middleware. When you create a deep agent with create_deep_agent , we automatically attach TodoListMiddleware , FilesystemMiddleware , and SubAgentMiddleware to your agent. graph LR\\n    Agent[create_deep_agent] --> Todo[TodoList]\\n    Agent --> FS[Filesystem]\\n    Agent --> Sub[SubAgent]\\n\\n    Todo --> Tools[Agent Tools]\\n    FS --> Tools\\n    Sub --> Tools\\n\\nUnderstand the middleware that powers deep agents\\n\\n\", \"Title: Deep Agents Middleware\\nLink: https://docs.langchain.com/oss/python/deepagents/middleware\\nContent: Deep agents are built with a modular middleware architecture. Deep agents have access to: A planning tool A filesystem for storing context and long-term memories The ability to spawn subagents Each feature is implemented as separate middleware. When you create a deep agent with create_deep_agent , we automatically attach TodoListMiddleware , FilesystemMiddleware , and SubAgentMiddleware to your agent. Middleware is composable—you can add as many or as few middleware to an agent as needed. You can use any middleware independently. The following sections explain what each middleware provides.\\n\\nDeep agents are built with a modular middleware architecture. Deep agents have access to: A planning tool A filesystem for storing context and long-term memories The ability to spawn subagents Each feature is implemented as separate middleware. When you create a deep agent with create_deep_agent , we automatically attach TodoListMiddleware , FilesystemMiddleware , and SubAgentMiddleware to your agent. graph LR\\n    Agent[create_deep_agent] --> Todo[TodoList]\\n    Agent --> FS[Filesystem]\\n    Agent --> Sub[SubAgent]\\n\\n    Todo --> Tools[Agent Tools]\\n    FS --> Tools\\n    Sub --> Tools\\n\\nUnderstand the middleware that powers deep agents\\n\\n\", \"Title: Provider-specific middleware\\nLink: https://docs.langchain.com/oss/python/integrations/middleware/index\\nContent: Provider-specific middleware\\nMiddleware designed for specific providers. Learn more about middleware. Provider Middleware available Anthropic Prompt caching, bash tool, text editor, memory, and file search OpenAI Content moderation Edit the source of this page on GitHub. Connect these docs programmatically to Claude, VSCode, and more via MCP for real-time answers.\\n\\nMiddleware designed for specific providers. Learn more about middleware. Provider Middleware available Anthropic Prompt caching, bash tool, text editor, memory, and file search OpenAI Content moderation Edit the source of this page on GitHub. Connect these docs programmatically to Claude, VSCode, and more via MCP for real-time answers.\\n\\n\", \"Title: Provider-specific middleware\\nLink: https://docs.langchain.com/oss/javascript/integrations/middleware/index\\nContent: Provider-specific middleware\\nMiddleware designed for specific providers. Learn more about middleware. Provider Middleware available Anthropic Prompt caching Edit the source of this page on GitHub. Connect these docs programmatically to Claude, VSCode, and more via MCP for real-time answers.\\n\\nMiddleware designed for specific providers. Learn more about middleware. Provider Middleware available Anthropic Prompt caching Edit the source of this page on GitHub. Connect these docs programmatically to Claude, VSCode, and more via MCP for real-time answers.\\n\\n\", \"Title: Middleware\\nLink: https://docs.langchain.com/oss/javascript/releases/langchain-v1\\nContent: Middleware is the defining feature of createAgent . It makes createAgent highly customizable, raising the ceiling for what you can build. Great agents require context engineering : getting the right information to the model at the right time. Middleware helps you control dynamic prompts, conversation summarization, selective tool access, state management, and guardrails through a composable abstraction.\\n\\nCustom middleware\\nYou can also build custom middleware to fit your specific needs. Build custom middleware by implementing any of these hooks using the create function: Hook When it runs Use cases beforeAgent Before calling the agent Load memory, validate input beforeModel Before each LLM call Update prompts, trim messages wrapModelCall Around each LLM call Intercept and modify requests/responses wrapToolCall Around each tool call Intercept and modify tool execution afterModel After each LLM response Validate output, apply guardrails afterAgent After agent completes Save results, cleanup Example custom middleware: For more information, see the complete middleware guide.\\n\\nCustom middleware\\nYou can also build custom middleware to fit your specific needs. Build custom middleware by implementing any of these hooks using the create function: Hook When it runs Use cases beforeAgent Before calling the agent Load memory, validate input beforeModel Before each LLM call Update prompts, trim messages wrapModelCall Around each LLM call Intercept and modify requests/responses wrapToolCall Around each tool call Intercept and modify tool execution afterModel After each LLM response Validate output, apply guardrails afterAgent After agent completes Save results, cleanup Example custom middleware: import { createMiddleware } from \\\"langchain\\\";\\n\\nconst contextSchema = z.object({\\n  userExpertise: z.enum([\\\"beginner\\\", \\\"expert\\\"]).default(\\\"beginner\\\"),\\n})\\n\\nconst expertiseBasedToolMiddleware = createMiddleware({\\n  wrapModelCall: async (request, handler) => {\\n    const userLevel = request.runtime.context.userExpertise;\\n    if (userLevel === \\\"expert\\\") {\\n      const tools = [advancedSearch, dataAnalysis];\\n      return handler(\\n        request.replace(\\\"openai:gpt-5\\\", tools)\\n      );\\n    }\\n    const tools = [simpleSearch, basicCalculator];\\n    return handler(\\n      request.replace(\\\"openai:gpt-5-nano\\\", tools)\\n    );\\n  },\\n});\\n\\nconst agent = createAgent({\\n  model: \\\"claude-sonnet-4-5-20250929\\\",\\n  tools: [simpleSearch, advancedSearch, basicCalculator, dataAnalysis],\\n  middleware: [expertiseBasedToolMiddleware],\\n  contextSchema,\\n});\\n\\n\"]\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I found a comprehensive overview of built-in middleware in LangChain. Here are the key points:\n",
      "\n",
      "1. **Overview**: LangChain provides prebuilt middleware for common use cases, which are production-ready and configurable.\n",
      "2. **Types of Middleware**:\n",
      "   - **Provider-specific Middleware**: Optimized for specific LLM providers, such as:\n",
      "     - **Anthropic**: Includes prompt caching, bash tool, text editor, memory, and file search middleware for Claude models.\n",
      "     - **OpenAI**: Includes content moderation middleware.\n",
      "   - **General Middleware**: Includes middleware like To-do List Middleware, which equips agents with task planning and tracking capabilities for complex multi-step tasks.\n",
      "\n",
      "For more details, you can refer to the following links:\n",
      "- [Built-in middleware (Python)](https://docs.langchain.com/oss/python/langchain/middleware/built-in)\n",
      "- [Built-in middleware (JavaScript)](https://docs.langchain.com/oss/javascript/langchain/middleware/built-in)\n",
      "\n",
      "STEP 2 DONE.\n",
      "\n",
      "### Step 3: For each middleware in that list, run a separate docs search focused on that middleware and update your notes\n",
      "\n",
      "Next, I will search for detailed information on each middleware identified in the previous step. I will start with the To-do List Middleware.\n",
      "Tool Calls:\n",
      "  SearchDocsByLangChain (call_Qnurpw79Qp44q1zFOOQRv8Oe)\n",
      " Call ID: call_Qnurpw79Qp44q1zFOOQRv8Oe\n",
      "  Args:\n",
      "    query: To-do List Middleware\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: SearchDocsByLangChain\n",
      "\n",
      "[\"Title: To-do list middleware\\nLink: https://docs.langchain.com/oss/python/deepagents/middleware\\nContent: Planning is integral to solving complex problems. If you've used Claude Code recently, you'll notice how it writes out a to-do list before tackling complex, multi-part tasks. You'll also notice how it can adapt and update this to-do list on the fly as more information comes in. TodoListMiddleware provides your agent with a tool specifically for updating this to-do list. Before and while it executes a multi-part task, the agent is prompted to use the write_todos tool to keep track of what it's doing and what still needs to be done.\\n\\nPlanning is integral to solving complex problems. If you've used Claude Code recently, you'll notice how it writes out a to-do list before tackling complex, multi-part tasks. You'll also notice how it can adapt and update this to-do list on the fly as more information comes in. TodoListMiddleware provides your agent with a tool specifically for updating this to-do list. Before and while it executes a multi-part task, the agent is prompted to use the write_todos tool to keep track of what it's doing and what still needs to be done. from langchain.agents import create_agent\\nfrom langchain.agents.middleware import TodoListMiddleware\\n\\n# TodoListMiddleware is included by default in create_deep_agent\\n# You can customize it if building a custom agent\\nagent = create_agent(\\n    model=\\\"claude-sonnet-4-5-20250929\\\",\\n    # Custom planning instructions can be added via middleware\\n    middleware=[\\n        TodoListMiddleware(\\n            system_prompt=\\\"Use the write_todos tool to...\\\"  # Optional: Custom addition to the system prompt\\n        ),\\n    ],\\n)\\n\\nDeep Agents Middleware\\nDeep agents are built with a modular middleware architecture. Deep agents have access to: A planning tool A filesystem for storing context and long-term memories The ability to spawn subagents Each feature is implemented as separate middleware. When you create a deep agent with create_deep_agent , we automatically attach TodoListMiddleware , FilesystemMiddleware , and SubAgentMiddleware to your agent. Middleware is composable—you can add as many or as few middleware to an agent as needed. You can use any middleware independently. The following sections explain what each middleware provides.\\n\\n\", \"Title: To-do list middleware\\nLink: https://docs.langchain.com/oss/javascript/deepagents/middleware\\nContent: Planning is integral to solving complex problems. If you've used Claude Code recently, you'll notice how it writes out a to-do list before tackling complex, multi-part tasks. You'll also notice how it can adapt and update this to-do list on the fly as more information comes in. TodoListMiddleware provides your agent with a tool specifically for updating this to-do list. Before and while it executes a multi-part task, the agent is prompted to use the write_todos tool to keep track of what it's doing and what still needs to be done.\\n\\nPlanning is integral to solving complex problems. If you've used Claude Code recently, you'll notice how it writes out a to-do list before tackling complex, multi-part tasks. You'll also notice how it can adapt and update this to-do list on the fly as more information comes in. TodoListMiddleware provides your agent with a tool specifically for updating this to-do list. Before and while it executes a multi-part task, the agent is prompted to use the write_todos tool to keep track of what it's doing and what still needs to be done. import { createAgent, todoListMiddleware } from \\\"langchain\\\";\\n\\n// todoListMiddleware is included by default in createDeepAgent\\n// You can customize it if building a custom agent\\nconst agent = createAgent({\\n  model: \\\"claude-sonnet-4-5-20250929\\\",\\n  middleware: [\\n    todoListMiddleware({\\n      // Optional: Custom addition to the system prompt\\n      systemPrompt: \\\"Use the write_todos tool to...\\\",\\n    }),\\n  ],\\n});\\n\\nDeep Agents Middleware\\nDeep agents are built with a modular middleware architecture. Deep agents have access to: A planning tool A filesystem for storing context and long-term memories The ability to spawn subagents Each feature is implemented as separate middleware. When you create a deep agent with create_deep_agent , we automatically attach TodoListMiddleware , FilesystemMiddleware , and SubAgentMiddleware to your agent. Middleware is composable—you can add as many or as few middleware to an agent as needed. You can use any middleware independently. The following sections explain what each middleware provides.\\n\\n\", \"Title: To-do list\\nLink: https://docs.langchain.com/oss/javascript/langchain/middleware/built-in\\nContent: Equip agents with task planning and tracking capabilities for complex multi-step tasks. To-do lists are useful for the following: Complex multi-step tasks requiring coordination across multiple tools. Long-running operations where progress visibility is important. This middleware automatically provides agents with a write_todos tool and system prompts to guide effective task planning. Watch this video guide demonstrating To-do List middleware behavior. No configuration options available (uses defaults).\\n\\nEquip agents with task planning and tracking capabilities for complex multi-step tasks. To-do lists are useful for the following: Complex multi-step tasks requiring coordination across multiple tools. Long-running operations where progress visibility is important. This middleware automatically provides agents with a write_todos tool and system prompts to guide effective task planning. import { createAgent, todoListMiddleware } from \\\"langchain\\\";\\n\\nconst agent = createAgent({\\n  model: \\\"gpt-4o\\\",\\n  tools: [readFile, writeFile, runTests],\\n  middleware: [todoListMiddleware()],\\n});\\n\\nBuilt-in middleware\\nLangChain provides prebuilt middleware for common use cases. Each middleware is production-ready and configurable for your specific needs.\\n\\n\", \"Title: To-do list\\nLink: https://docs.langchain.com/oss/python/langchain/middleware/built-in\\nContent: Equip agents with task planning and tracking capabilities for complex multi-step tasks. To-do lists are useful for the following: Complex multi-step tasks requiring coordination across multiple tools. Long-running operations where progress visibility is important. This middleware automatically provides agents with a write_todos tool and system prompts to guide effective task planning. API reference: TodoListMiddleware Watch this video guide demonstrating To-do List middleware behavior. system_prompt {\\\"body\\\":\\\"system_prompt\\\",\\\"type\\\":\\\"string\\\"}\\n\\nEquip agents with task planning and tracking capabilities for complex multi-step tasks. To-do lists are useful for the following: Complex multi-step tasks requiring coordination across multiple tools. Long-running operations where progress visibility is important. This middleware automatically provides agents with a write_todos tool and system prompts to guide effective task planning. API reference: TodoListMiddleware Watch this video guide demonstrating To-do List middleware behavior. system_prompt Custom system prompt for guiding todo usage. Uses built-in prompt if not specified. tool_description {\\\"body\\\":\\\"tool_description\\\",\\\"type\\\":\\\"string\\\"}\\n\\nEquip agents with task planning and tracking capabilities for complex multi-step tasks. To-do lists are useful for the following: Complex multi-step tasks requiring coordination across multiple tools. Long-running operations where progress visibility is important. This middleware automatically provides agents with a write_todos tool and system prompts to guide effective task planning. API reference: TodoListMiddleware Watch this video guide demonstrating To-do List middleware behavior. system_prompt Custom system prompt for guiding todo usage. Uses built-in prompt if not specified. tool_description Custom description for the write_todos tool. Uses built-in description if not specified.\\n\\n\", \"Title: How to add custom middleware\\nLink: https://docs.langchain.com/langsmith/custom-middleware\\nContent: When deploying agents to LangSmith, you can add custom middleware to your server to handle concerns like logging request metrics, injecting or checking headers, and enforcing security policies without modifying core server logic. This works the same way as adding custom routes . You just need to provide your own Starlette app (including FastAPI , FastHTML and other compatible apps). Adding middleware lets you intercept and modify requests and responses globally across your deployment, whether they're hitting your custom endpoints or the built-in LangSmith APIs. Below is an example using FastAPI. \\\"Python only\\\"\\nWe currently only support custom middleware in Python deployments with langgraph-api>=0.0.26.\\n\\nHow to add custom middleware\\nWhen deploying agents to LangSmith, you can add custom middleware to your server to handle concerns like logging request metrics, injecting or checking headers, and enforcing security policies without modifying core server logic. This works the same way as adding custom routes . You just need to provide your own Starlette app (including FastAPI , FastHTML and other compatible apps). Adding middleware lets you intercept and modify requests and responses globally across your deployment, whether they're hitting your custom endpoints or the built-in LangSmith APIs. Below is an example using FastAPI. \\\"Python only\\\"\\nWe currently only support custom middleware in Python deployments with langgraph-api>=0.0.26.\\n\\nCustomize middleware ordering\\nBy default, custom middleware runs before authentication logic. To run custom middleware after authentication, set middleware_order to auth_first in your http configuration. (This customization is supported starting with API server v0.4.35 and later.)\\n\\n\", \"Title: Provider-specific middleware\\nLink: https://docs.langchain.com/oss/python/integrations/middleware/index\\nContent: Provider-specific middleware\\nMiddleware designed for specific providers. Learn more about middleware. Provider Middleware available Anthropic Prompt caching, bash tool, text editor, memory, and file search OpenAI Content moderation Edit the source of this page on GitHub. Connect these docs programmatically to Claude, VSCode, and more via MCP for real-time answers.\\n\\nMiddleware designed for specific providers. Learn more about middleware. Provider Middleware available Anthropic Prompt caching, bash tool, text editor, memory, and file search OpenAI Content moderation Edit the source of this page on GitHub. Connect these docs programmatically to Claude, VSCode, and more via MCP for real-time answers.\\n\\n\", \"Title: Provider-specific middleware\\nLink: https://docs.langchain.com/oss/javascript/integrations/middleware/index\\nContent: Provider-specific middleware\\nMiddleware designed for specific providers. Learn more about middleware. Provider Middleware available Anthropic Prompt caching Edit the source of this page on GitHub. Connect these docs programmatically to Claude, VSCode, and more via MCP for real-time answers.\\n\\nMiddleware designed for specific providers. Learn more about middleware. Provider Middleware available Anthropic Prompt caching Edit the source of this page on GitHub. Connect these docs programmatically to Claude, VSCode, and more via MCP for real-time answers.\\n\\n\", \"Title: Custom middleware\\nLink: https://docs.langchain.com/oss/python/langchain/middleware/custom\\nContent: Build custom middleware by implementing hooks that run at specific points in the agent execution flow.\\n\\nCustom middleware\\nBuild custom middleware by implementing hooks that run at specific points in the agent execution flow.\\n\\nCreate middleware\\nYou can create middleware in two ways: Decorator-based middleware Quick and simple for single-hook middleware. Use decorators to wrap individual functions. Class-based middleware More powerful for complex middleware with multiple hooks or configuration.\\n\\n\", \"Title: Custom middleware\\nLink: https://docs.langchain.com/oss/javascript/langchain/middleware/custom\\nContent: Build custom middleware by implementing hooks that run at specific points in the agent execution flow.\\n\\nCustom middleware\\nBuild custom middleware by implementing hooks that run at specific points in the agent execution flow.\\n\\nCreate middleware\\nUse the createMiddleware function to define custom middleware: import { createMiddleware } from \\\"langchain\\\";\\n\\nconst loggingMiddleware = createMiddleware({\\n  name: \\\"LoggingMiddleware\\\",\\n  beforeModel: (state) => {\\n    console.log(`About to call model with ${state.messages.length} messages`);\\n    return;\\n  },\\n  afterModel: (state) => {\\n    const lastMessage = state.messages[state.messages.length - 1];\\n    console.log(`Model returned: ${lastMessage.content}`);\\n    return;\\n  },\\n});\\n\\n\", \"Title: Middleware\\nLink: https://docs.langchain.com/oss/javascript/langchain/agents\\nContent: Middleware provides powerful extensibility for customizing agent behavior at different stages of execution. You can use middleware to: Process state before the model is called (e.g., message trimming, context injection) Modify or validate the model's response (e.g., guardrails, content filtering) Handle tool execution errors with custom logic Implement dynamic model selection based on state or context Add custom logging, monitoring, or analytics Middleware integrates seamlessly into the agent's execution, allowing you to intercept and modify data flow at key points without changing the core agent logic. For comprehensive middleware documentation including hooks like beforeModel , afterModel , and wrapToolCall , see Middleware. Edit the source of this page on GitHub. Connect these docs programmatically to Claude, VSCode, and more via MCP for real-time answers.\\n\\nDynamic system prompt\\nFor more advanced use cases where you need to modify the system prompt based on runtime context or agent state, you can use middleware. For more details on message types and formatting, see Messages . For comprehensive middleware documentation, see .\\n\\nDynamic model\\nDynamic models are selected at runtime based on the current state and context. This enables sophisticated routing logic and cost optimization. To use a dynamic model, create middleware with wrapModelCall that modifies the model in the request: import { ChatOpenAI } from \\\"@langchain/openai\\\";\\nimport { createAgent, create } from \\\"langchain\\\";\\n\\nconst basicModel = new ChatOpenAI({ model: \\\"gpt-4o-mini\\\" });\\nconst advancedModel = new ChatOpenAI({ model: \\\"gpt-4o\\\" });\\n\\nconst dynamicModelSelection = createMiddleware({\\n  name: \\\"DynamicModelSelection\\\",\\n  wrapModelCall: (request, handler) => {\\n    // Choose model based on conversation complexity\\n    const messageCount = request.messages.length;\\n\\n    return handler({\\n        ...request,\\n        model: messageCount > 10 ? advancedModel : basicModel,\\n    });\\n  },\\n});\\n\\nconst agent = createAgent({\\n  model: \\\"gpt-4o-mini\\\", // Base model (used when messageCount ≤ 10)\\n  tools,\\n  middleware: [dynamicModelSelection],\\n});\\n\\n\"]\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Here are the details I found about the **To-do List Middleware**:\n",
      "\n",
      "1. **Purpose**: The To-do List Middleware is designed to assist agents in planning and tracking complex multi-step tasks. It allows agents to maintain a dynamic to-do list that can be updated as tasks progress.\n",
      "\n",
      "2. **Functionality**:\n",
      "   - The middleware prompts the agent to use a `write_todos` tool to keep track of ongoing tasks and what still needs to be done.\n",
      "   - It is particularly useful for tasks that require coordination across multiple tools or for long-running operations where visibility into progress is important.\n",
      "\n",
      "3. **Implementation**:\n",
      "   - The middleware can be included by default when creating a deep agent using `create_deep_agent`.\n",
      "   - Customization options are available, such as adding a custom system prompt to guide the usage of the to-do list.\n",
      "\n",
      "4. **Example Code**:\n",
      "   ```python\n",
      "   from langchain.agents import create_agent\n",
      "   from langchain.agents.middleware import TodoListMiddleware\n",
      "\n",
      "   agent = create_agent(\n",
      "       model=\"claude-sonnet-4-5-20250929\",\n",
      "       middleware=[\n",
      "           TodoListMiddleware(\n",
      "               system_prompt=\"Use the write_todos tool to...\"\n",
      "           ),\n",
      "       ],\n",
      "   )\n",
      "   ```\n",
      "\n",
      "5. **Links for Reference**:\n",
      "   - [To-do List Middleware Documentation (Python)](https://docs.langchain.com/oss/python/deepagents/middleware)\n",
      "   - [To-do List Middleware Documentation (JavaScript)](https://docs.langchain.com/oss/javascript/deepagents/middleware)\n",
      "\n",
      "Next, I will search for information on the **Provider-specific Middleware**.\n",
      "Tool Calls:\n",
      "  SearchDocsByLangChain (call_fqeTB8cFbFNXv49Z1flw6iNE)\n",
      " Call ID: call_fqeTB8cFbFNXv49Z1flw6iNE\n",
      "  Args:\n",
      "    query: Provider-specific middleware\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: SearchDocsByLangChain\n",
      "\n",
      "[\"Title: Provider-specific middleware\\nLink: https://docs.langchain.com/oss/python/integrations/middleware/index\\nContent: Provider-specific middleware\\nMiddleware designed for specific providers. Learn more about middleware. Provider Middleware available Anthropic Prompt caching, bash tool, text editor, memory, and file search OpenAI Content moderation Edit the source of this page on GitHub. Connect these docs programmatically to Claude, VSCode, and more via MCP for real-time answers.\\n\\nMiddleware designed for specific providers. Learn more about middleware. Provider Middleware available Anthropic Prompt caching, bash tool, text editor, memory, and file search OpenAI Content moderation Edit the source of this page on GitHub. Connect these docs programmatically to Claude, VSCode, and more via MCP for real-time answers.\\n\\n\", \"Title: Provider-specific middleware\\nLink: https://docs.langchain.com/oss/javascript/integrations/middleware/index\\nContent: Provider-specific middleware\\nMiddleware designed for specific providers. Learn more about middleware. Provider Middleware available Anthropic Prompt caching Edit the source of this page on GitHub. Connect these docs programmatically to Claude, VSCode, and more via MCP for real-time answers.\\n\\nMiddleware designed for specific providers. Learn more about middleware. Provider Middleware available Anthropic Prompt caching Edit the source of this page on GitHub. Connect these docs programmatically to Claude, VSCode, and more via MCP for real-time answers.\\n\\n\", \"Title: Provider-specific middleware\\nLink: https://docs.langchain.com/oss/python/langchain/middleware/built-in\\nContent: These middleware are optimized for specific LLM providers. See each provider's documentation for full details and examples. Anthropic Prompt caching, bash tool, text editor, memory, and file search middleware for Claude models. OpenAI Content moderation middleware for OpenAI models. Edit the source of this page on GitHub. Connect these docs programmatically to Claude, VSCode, and more via MCP for real-time answers.\\n\\nBuilt-in middleware\\nLangChain provides prebuilt middleware for common use cases. Each middleware is production-ready and configurable for your specific needs.\\n\\nBuilt-in middleware\\nPrebuilt middleware for common agent use cases\\n\\n\", \"Title: Provider-specific middleware\\nLink: https://docs.langchain.com/oss/javascript/langchain/middleware/built-in\\nContent: These middleware are optimized for specific LLM providers. See each provider's documentation for full details and examples. Anthropic Prompt caching, bash tool, text editor, memory, and file search middleware for Claude models. Edit the source of this page on GitHub. Connect these docs programmatically to Claude, VSCode, and more via MCP for real-time answers.\\n\\nBuilt-in middleware\\nLangChain provides prebuilt middleware for common use cases. Each middleware is production-ready and configurable for your specific needs.\\n\\nBuilt-in middleware\\nPrebuilt middleware for common agent use cases\\n\\n\", \"Title: How to add custom middleware\\nLink: https://docs.langchain.com/langsmith/custom-middleware\\nContent: When deploying agents to LangSmith, you can add custom middleware to your server to handle concerns like logging request metrics, injecting or checking headers, and enforcing security policies without modifying core server logic. This works the same way as adding custom routes . You just need to provide your own Starlette app (including FastAPI , FastHTML and other compatible apps). Adding middleware lets you intercept and modify requests and responses globally across your deployment, whether they're hitting your custom endpoints or the built-in LangSmith APIs. Below is an example using FastAPI. \\\"Python only\\\"\\nWe currently only support custom middleware in Python deployments with langgraph-api>=0.0.26.\\n\\nHow to add custom middleware\\nWhen deploying agents to LangSmith, you can add custom middleware to your server to handle concerns like logging request metrics, injecting or checking headers, and enforcing security policies without modifying core server logic. This works the same way as adding custom routes . You just need to provide your own Starlette app (including FastAPI , FastHTML and other compatible apps). Adding middleware lets you intercept and modify requests and responses globally across your deployment, whether they're hitting your custom endpoints or the built-in LangSmith APIs. Below is an example using FastAPI. \\\"Python only\\\"\\nWe currently only support custom middleware in Python deployments with langgraph-api>=0.0.26.\\n\\nCustomize middleware ordering\\nBy default, custom middleware runs before authentication logic. To run custom middleware after authentication, set middleware_order to auth_first in your http configuration. (This customization is supported starting with API server v0.4.35 and later.)\\n\\n\", \"Title: Custom middleware\\nLink: https://docs.langchain.com/oss/python/langchain/middleware/custom\\nContent: Build custom middleware by implementing hooks that run at specific points in the agent execution flow.\\n\\nCustom middleware\\nBuild custom middleware by implementing hooks that run at specific points in the agent execution flow.\\n\\nCreate middleware\\nYou can create middleware in two ways: Decorator-based middleware Quick and simple for single-hook middleware. Use decorators to wrap individual functions. Class-based middleware More powerful for complex middleware with multiple hooks or configuration.\\n\\n\", \"Title: Custom middleware\\nLink: https://docs.langchain.com/oss/javascript/langchain/middleware/custom\\nContent: Build custom middleware by implementing hooks that run at specific points in the agent execution flow.\\n\\nCustom middleware\\nBuild custom middleware by implementing hooks that run at specific points in the agent execution flow.\\n\\nCreate middleware\\nUse the createMiddleware function to define custom middleware: import { createMiddleware } from \\\"langchain\\\";\\n\\nconst loggingMiddleware = createMiddleware({\\n  name: \\\"LoggingMiddleware\\\",\\n  beforeModel: (state) => {\\n    console.log(`About to call model with ${state.messages.length} messages`);\\n    return;\\n  },\\n  afterModel: (state) => {\\n    const lastMessage = state.messages[state.messages.length - 1];\\n    console.log(`Model returned: ${lastMessage.content}`);\\n    return;\\n  },\\n});\\n\\n\", \"Title: Deep Agents Middleware\\nLink: https://docs.langchain.com/oss/javascript/deepagents/middleware\\nContent: Deep agents are built with a modular middleware architecture. Deep agents have access to: A planning tool A filesystem for storing context and long-term memories The ability to spawn subagents Each feature is implemented as separate middleware. When you create a deep agent with create_deep_agent , we automatically attach TodoListMiddleware , FilesystemMiddleware , and SubAgentMiddleware to your agent. Middleware is composable—you can add as many or as few middleware to an agent as needed. You can use any middleware independently. The following sections explain what each middleware provides.\\n\\nUnderstand the middleware that powers deep agents\\n\\nDeep agents are built with a modular middleware architecture. Deep agents have access to: A planning tool A filesystem for storing context and long-term memories The ability to spawn subagents Each feature is implemented as separate middleware. When you create a deep agent with create_deep_agent , we automatically attach TodoListMiddleware , FilesystemMiddleware , and SubAgentMiddleware to your agent. graph LR\\n    Agent[create_deep_agent] --> Todo[TodoList]\\n    Agent --> FS[Filesystem]\\n    Agent --> Sub[SubAgent]\\n\\n    Todo --> Tools[Agent Tools]\\n    FS --> Tools\\n    Sub --> Tools\\n\\n\", \"Title: Deep Agents Middleware\\nLink: https://docs.langchain.com/oss/python/deepagents/middleware\\nContent: Deep agents are built with a modular middleware architecture. Deep agents have access to: A planning tool A filesystem for storing context and long-term memories The ability to spawn subagents Each feature is implemented as separate middleware. When you create a deep agent with create_deep_agent , we automatically attach TodoListMiddleware , FilesystemMiddleware , and SubAgentMiddleware to your agent. Middleware is composable—you can add as many or as few middleware to an agent as needed. You can use any middleware independently. The following sections explain what each middleware provides.\\n\\nUnderstand the middleware that powers deep agents\\n\\nDeep agents are built with a modular middleware architecture. Deep agents have access to: A planning tool A filesystem for storing context and long-term memories The ability to spawn subagents Each feature is implemented as separate middleware. When you create a deep agent with create_deep_agent , we automatically attach TodoListMiddleware , FilesystemMiddleware , and SubAgentMiddleware to your agent. graph LR\\n    Agent[create_deep_agent] --> Todo[TodoList]\\n    Agent --> FS[Filesystem]\\n    Agent --> Sub[SubAgent]\\n\\n    Todo --> Tools[Agent Tools]\\n    FS --> Tools\\n    Sub --> Tools\\n\\n\", \"Title: Middleware\\nLink: https://docs.langchain.com/oss/javascript/releases/langchain-v1\\nContent: Middleware is the defining feature of createAgent . It makes createAgent highly customizable, raising the ceiling for what you can build. Great agents require context engineering : getting the right information to the model at the right time. Middleware helps you control dynamic prompts, conversation summarization, selective tool access, state management, and guardrails through a composable abstraction.\\n\\nCustom middleware\\nYou can also build custom middleware to fit your specific needs. Build custom middleware by implementing any of these hooks using the create function: Hook When it runs Use cases beforeAgent Before calling the agent Load memory, validate input beforeModel Before each LLM call Update prompts, trim messages wrapModelCall Around each LLM call Intercept and modify requests/responses wrapToolCall Around each tool call Intercept and modify tool execution afterModel After each LLM response Validate output, apply guardrails afterAgent After agent completes Save results, cleanup Example custom middleware: import { createMiddleware } from \\\"langchain\\\";\\n\\nconst contextSchema = z.object({\\n  userExpertise: z.enum([\\\"beginner\\\", \\\"expert\\\"]).default(\\\"beginner\\\"),\\n})\\n\\nconst expertiseBasedToolMiddleware = createMiddleware({\\n  wrapModelCall: async (request, handler) => {\\n    const userLevel = request.runtime.context.userExpertise;\\n    if (userLevel === \\\"expert\\\") {\\n      const tools = [advancedSearch, dataAnalysis];\\n      return handler(\\n        request.replace(\\\"openai:gpt-5\\\", tools)\\n      );\\n    }\\n    const tools = [simpleSearch, basicCalculator];\\n    return handler(\\n      request.replace(\\\"openai:gpt-5-nano\\\", tools)\\n    );\\n  },\\n});\\n\\nconst agent = createAgent({\\n  model: \\\"claude-sonnet-4-5-20250929\\\",\\n  tools: [simpleSearch, advancedSearch, basicCalculator, dataAnalysis],\\n  middleware: [expertiseBasedToolMiddleware],\\n  contextSchema,\\n});\\n\\nCustom middleware\\nYou can also build custom middleware to fit your specific needs. Build custom middleware by implementing any of these hooks using the create function: Hook When it runs Use cases beforeAgent Before calling the agent Load memory, validate input beforeModel Before each LLM call Update prompts, trim messages wrapModelCall Around each LLM call Intercept and modify requests/responses wrapToolCall Around each tool call Intercept and modify tool execution afterModel After each LLM response Validate output, apply guardrails afterAgent After agent completes Save results, cleanup Example custom middleware: For more information, see the complete middleware guide.\\n\\n\"]\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Model call limits exceeded: run limit (3/3)\n"
     ]
    }
   ],
   "source": [
    "for m in response['messages']:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the difference? The agent got cut off after 3 model calls.\n",
    "\n",
    "With `exit_behavior=\"end\"`, it stopped gracefully. The conversation just ends – no crash, no error, just a polite \"that's enough for now.\"\n",
    "\n",
    "The agent might not have finished its research, but at least your budget is intact."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. When Would You Use This?\n",
    "\n",
    "A few real-world scenarios:\n",
    "\n",
    "- **Development & testing** – Cap calls low so you don't burn through credits while debugging\n",
    "- **Cost control in production** – Set reasonable limits per request to prevent runaway costs\n",
    "- **Safety net** – Even if your agent logic is solid, limits provide a backstop against unexpected loops\n",
    "\n",
    "There's also `thread_limit` if you want to cap calls across an entire conversation thread (multiple invocations). But `run_limit` handles the most common case: keeping a single request from going off the rails."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. The Takeaway\n",
    "\n",
    "Agents are enthusiastic. Sometimes too enthusiastic.\n",
    "\n",
    "`ModelCallLimitMiddleware` is a simple, one-line addition that gives you control over how much work an agent can do. It's not about making the agent worse – it's about making it predictable.\n",
    "\n",
    "In production, predictability is worth its weight in gold. Or in this case, in API credits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference Links\n",
    "\n",
    "**1. ModelCallLimitMiddleware – Python docs**\n",
    "\n",
    "https://docs.langchain.com/oss/python/langchain/middleware/built-in\n",
    "\n",
    "→ Full parameter reference including `thread_limit`, `run_limit`, and `exit_behavior`.\n",
    "\n",
    "**2. Middleware in LangChain 1.0**\n",
    "\n",
    "https://docs.langchain.com/oss/python/langchain/middleware\n",
    "\n",
    "→ High-level guide to the middleware system and how to combine multiple middlewares."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
